<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"> 
<!--
Design by TEMPLATED
http://templated.co
Released for free under the Creative Commons Attribution License

Name       : Skeleton 
Description: A two-column, fixed-width design with dark color scheme.
Version    : 1.0
Released   : 20130902

--> 
<html xmlns="http://www.w3.org/1999/xhtml"> 
    <head> 
        <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/> 
        <title>Darren Cosker's Home Page</title>         
        <meta name="keywords" content=""/> 
        <meta name="description" content=""/> 
        <link href="../css/theme.css" rel="stylesheet" type="text/css"> 
        <link href="http://fonts.googleapis.com/css?family=Source+Sans+Pro:200,300,400,600,700,900" rel="stylesheet"/> 
        <link href="default.css" rel="stylesheet" type="text/css" media="all"/> 
        <link href="fonts.css" rel="stylesheet" type="text/css" media="all"/> 
        <!--[if IE 6]><link href="default_ie6.css" rel="stylesheet" type="text/css" /><![endif]-->         
    </head>     
    <body> 
        <div id="page" class="container"> 
            <div id="header"> 
                <div id="logo"> 
                    <img src="images/darren_6.jpg" alt="" width="135" height="135"/> 
                    <h2><a href="#">Prof. Darren Cosker</a></h2> 
                    <p><font color="white">D.P.Cosker@bath.ac.uk</font></p> 
                    <p><font color="white">Professor/Royal Society Industry Fellow</font></p> <span><br/> </span> 
                </div>                 
                <div id="menu"> 
                    <ul> 
                        <li> <a href="http://www.camera.ac.uk">CAMERA</a> 
                        </li>                         
                        <li> <a href="#areas">RESEARCH AREAS</a> 
                        </li>                         
                        <li> <a href="#funding">FUNDING</a> 
                        </li>                         
                        <li> <a href="#team">TEAM</a> 
                        </li>                         
                        <li> <a href="#papers">PAPERS</a> 
                        </li>                         
                        <li> <a href="#commercial">COMMERCIAL</a> 
                        </li>                         
                        <li> <a href="#datasets">CODE+DATA</a> 
                        </li>                         
                        <li> <a href="#archive">ARCHIVE</a> 
                        </li>                         
                    </ul>                     
                </div>                 
            </div>             
            <div id="main" align="center"> 
                <div id="banner"> 
                    <img src="images/studio_2.jpg" alt="" class="image-full"/> 
                </div>                 
                <!--
		<table width="200" border="0" align="center" cellpadding="10">
  <tbody>
    <tr>
      <td><img src="images/darren_6.jpg" alt="" width="150" height="150" /></td>
      <td>Prof. Darren Cosker
          <p>D.P.Cosker@bath.ac.uk</p>
          <p>Professor/Royal Society Industry Fellow</p></td>
    </tr>
  </tbody>
</table>
-->                 <a href="http://www.camera.ac.uk"><img src="images/CAMERA_C_logo.jpg" width="55" height="50" width="50" height="50" alt=""/></a> <a href="https://github.com/dopomoc"><img src="images/github_2.png" width="50" height="50"></a> <a href="https://twitter.com/dopomoc"><img src="images/twitter.png" width="50" height="50" alt=""/></a> <a href="https://www.linkedin.com/in/darren-cosker-61271413?trk=hp-identity-name"><img src="images/linkedin.png" width="50" height="50" alt=""/></a> 
                <p><a href="http://www.camera.ac.uk">CAMERA</a> | <a href="#areas">Research Areas</a> | <a href="#funding">Research Funding</a> |  <a href="#team">Research Team</a> |  <a href="#papers">Research Papers</a> | <a href="#commercial">Commercial Projects</a> | <a href="#datasets">Code and Data </a> | <a href="#archive">Archive</a></p> 
                <div id="welcome" style="text-align: left"> 
                    <div class="title"> 
                        <!--<h2>Home</h2>-->                         
                    </div>                     
                    <p>I am a Professor (now part time) in Computer Science at the University of Bath (since 2017) and a Principal Scientist at Microsoft (Mixed Reality and AI). Previously I was a Lecturer (Assistant Prof./US) in Computer Science in 2012, and a Reader (Associate Prof./US) from 2014. I have been fortunate enough to be awarded two previous Research Fellowships: Royal Academy of Engineering, 2007-2012, Royal Society Industry Fellowship (with Double Negative Visual Effects), 2012-2016.</p> 
                    <p> In 2015 I founded and was Director of the <a>Centre for the Analysis of Motion, Entertainment Research and Applications (CAMERA)</a>, funded by EPSRC/AHRC, with partner contributions from The Imaginarium, The Foundry, British Skeleton, Ministry of Defence and British Maritime Technologies. In 2020 I led the successful re-funding of CAMERA for another 5 years (EPSRC) and helped secure the £45m+ MyWorld project in the South West of the UK. <p/> <p> In 2021 I stepped down as Director of CAMERA (although I still contribute as an investigator) to join Microsoft (based in Cambridge, UK) as an FTE - a split role with the University of Bath. At Microsoft I work in the Mixed Reality team - working on technologies for 'Presence'.</p> <p>Although primarily embedded in Visual Computing and AI, I'm a multi-disciplinary scientist/researcher interested in problems that cut across disciplines - resulting in papers published in diverse disciplines such as animation, vision, emotion perception and sport. I'm also interested in democratising technology to enable people to do great work - whether they are researchers or an every day person just wanting to be more productive or have fun. Past applications of my research have been in areas such as human motion analysis, recognition and synthesis, but I'm interested in any problem that involves understanding and modeling data. Applications have in the past been across creative industries (e.g. we worked on some great projects with the BBC and Aardman - <a href="https://www.bbc.co.uk/news/resources/idt-sh/is_anna_ok">'Is Anna OK?'</a> and <a href="https://en.bandainamcoent.eu/11-11-memories-retold/11-11-memories-retold">'11:11 Memories Retold'</a>), healthcare (e.g. AI to manage disease) and sport (e.g. markerless biomechanical analysis), but I'm always looking for new interesting ways to apply my research.</p> <p>I'm always looking for bright potential PhD students or post-doctoral reseaerchers to work on projects in CAMERA. Please get in touch if people are interested even if positions are not advertised. Apologies in advance if I don't respond to every email but be assured that if you have a strong background and the right project looks to be coming up I will be in touch!</p> <!--<p><font color="#FF0004"><b>PhD/Post-Doctoral Researcher Opportunities!</b></font>I am currently looking for a post-doctoral researcher with a background in 3D computer vision, ideally with application areas/interests in real time tracking. There are also PhD opportunities within CAMERA in several areas across motion capture and animation. Experience/interest in Computer Vision, Graphics and/or machine learning is desirable. Please email me for enquiries.</p>-->  
                </div>
                <div class="title"> 
                    <h2>Ship it! Production and Commercial Projects</h2> <a name="commercial"></a> 
                </div>
                <div id="commercial"> 
                    <p>I have always been a believer that you shouldn't do research and then just 'throw it over the wall' for others to take to real users and customers. In fact, most of the hard - and really interesting work - starts when you get your great reseach ideas into real people hands. Then you find out a few things - does it really work on real data? Is it robust? You will discover dozens of things that are wrong,  could be improved, and even better a set of new problems. At Microsoft I have been fortunate to get research into the hands of millions of customers through Mesh and Teams. In <a href="http://www.camera.ac.uk">CAMERA</a> I created a model whereby we translated research into impact by creating tools from our research, deploying them into our studio, and then delivering projects to clients that leverage these tools. Below is a snapshot of some of the projects I have been fortunate enough to be involved in!</p> 
                    <table width="484" border="0" align="center"> 
                        <tbody> 
                            <tr> 
                                <td> 
                                    <img src="images/Cosmos.jpg" alt="" width="220" height="130"/> 
                                </td>                                 
                                <td> 
                                    <img src="images/Aardman.jpg" alt="" width="260" height="120"/> 
                                </td>                                 
                            </tr>                             
                            <tr> 
                                <td>Cosmos Within Us - with Satore Studios</td> 
                                <td>11:11 Memories Retold - with Aardman and Bandai Namco</td> 
                            </tr>                             
                            <tr> 
                                <td> 
                                    <img src="images/BBC.jpg" alt="" width="240" height="110"/> 
                                </td>                                 
                                <td> 
                                    <img src="images/REWIND.jpg" alt="" width="240" height="110"/> 
                                </td>                                 
                            </tr>                             
                            <tr> 
                                <td>'Is Anna OK?' - with BBC and Aardman</td> 
                                <td>Magic Butterfly - with REWIND and WNO</td> 
                            </tr>                             
                        </tbody>                         
                    </table>                     
                </div>                 
                <div class="title"> 
                    <h2>Research Themes</h2> <a name="areas"></a> 
                </div>                 
                <p>I have always been interested in people - and understanding how they move so we can model this and then learn to create better animation systems. This involves peoples faces, bodies, the environment, each other - and then the application to video games, movies, remote communication. I believe motion should always be higher fidelity than appearance - and poor appearance and great motion will always be better than great appearance and poor motion.</p>
                <p>Along this direction I have explored many projects in the past - both in academia and industry - and established new teams as well as multiple commercial grade motion capture production studios. Below is an overview of my research based on different areas I have worked on. It is a representative list of papers only by broad topic area - for a full list see the <a href="#papers">research papers</a> section.</p> 
                <table width="484" border="0" align="center"> 
                    <tbody> 
                        <h3>Facial Analysis and Synthesis</h3> 
                        <tr> 
                            <td width="88"> 
                                <img src="images/CGF_2019.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                            <td> 
                                <img src="images/rigidity.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                            <td width="89"> 
                                <img src="images/MIG_2016.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                            <td width="89"> 
                                <img src="images/GI_2016.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                            <td width="90"> 
                                <img src="images/iccv.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                            <td width="90"> 
                                <img src="images/Vedran_IEEE.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                            <td> 
                                <img src="images/Trellis.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                        </tr>                         
                        <tr> 
                            <td>Evolutionary Facial Animation - CGF 2019</td> 
                            <td>Content Aware Deformation - CVMP 2015</td> 
                            <td>Procedural Facial Animation - CGF 2018</td> 
                            <td>Reading Between the Dots: Facial Capture- GI 2017</td> 
                            <td>Dynamic Morphable Models: D3DFACS - ICCV 2011</td> 
                            <td>4D Facial Movement for Biometrics - IEEE SMC 2010</td> 
                            <td>Speech Driven Facial Animation - ICPR 2004</td> 
                        </tr>                         
                    </tbody>                     
                </table>                 
                <table width="484" border="0" align="center"> 
                    <tbody> 
                        <h3>Performance Capture and Animation</h3> 
                        <tr> 
                            <td> 
                                <img src="images/RGBDDog.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                            <td> 
                                <img src="images/CGI_2019.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                            <td> 
                                <img src="images/SPORT_2018.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                            <td> 
                                <img src="images/FOOT_2018.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                            <td> 
                                <img src="images/MIG_2018.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                        </tr>                         
                        <tr> 
                            <td>RGBD-Dog: Predicting Canine Pose from RGBD  - CVPR 2020</td> 
                            <td>Scale Aware Performance Retargeting - CGI 2019</td> 
                            <td>Markerless Motion Capture Survey - Sports Medicine 2018</td> 
                            <td>Markerless Sprint Analysis - WACV 2018</td> 
                            <td>Elastic Deformation - MIG 2018</td> 
                        </tr>                         
                    </tbody>                     
                </table>                 
                <table width="484" border="0" align="center"> 
                    <tbody> 
                        <h3>Image and Video Processing</h3> 
                        <tr> 
                            <td> 
                                <img src="images/NIPS_2018.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                            <td> 
                                <img src="images/CVPR_2018.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                            <td> 
                                <img src="images/blur_robust.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                            <td> 
                                <img src="images/inferring.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                            <td> 
                                <img src="images/bmvc14.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                            <td> 
                                <img src="images/anchor_patch.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                        </tr>                         
                        <tr> 
                            <td>Image to Image Translation - NeurIPS 2018</td> 
                            <td>Multi-task Learning - CVPR 2018</td> 
                            <td>Blur Robust Robust Optical Flow - PR 2017</td> 
                            <td>Inferring Focal Length - CG 2015</td> 
                            <td>Shadow Removal - BMVC 2014</td> 
                            <td>Robust Feature Tracking - ACCV 2013</td> 
                        </tr>                         
                        <tr> 
                            <td></td> 
                            <td> 
                                <img src="images/digipro.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                            <td> 
                                <img src="images/LME_Faces.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                            <td> 
                                <img src="images/RAL_2016.bmp" alt="" width="100" height="80"/> 
                            </td>                             
                            <td> 
                                <img src="images/TVCG_Water.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                        </tr>                         
                        <tr> 
                            <td></td> 
                            <td>Camera Tracking in Visual Effects - DigiPro 2016</td> 
                            <td>Mesh based Optical Flow - CVPR 2013</td> 
                            <td>Non-Rigid Optical Flow Ground Truth - RAL 2016</td> 
                            <td>Water Reconstruction from Video - TVCG 2015</td> 
                        </tr>                         
                    </tbody>                     
                </table>                 
                <table width="484" border="0" align="center"> 
                    <tbody> 
                        <h3>Applied Perception for Vision and Graphics</h3> 
                        <tr> 
                            <td> 
                                <img src="images/APGV_2010.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                            <td> 
                                <img src="images/smile_traj_.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                            <td> 
                                <img src="images/McGurk.png" alt="" width="100" height="80"/> 
                            </td>                             
                            <td> 
                                <img src="images/sap_2016.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                            <td> 
                                <img src="images/hci.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                        </tr>                         
                        <tr> 
                            <td>Nonlinear 4D Facial Perception - ACM APGV / SAP 2011</td> 
                            <td>Facial Dynamics and Trustworthiness - Emotion 2008 </td> 
                            <td>Perceptual Evaluation of Video Based Facial Animation - ACM TAP 2005</td> 
                            <td>Evaluation of Foveated Rendering Methods - ACM SAP 2016</td> 
                            <td>Evaluation of Gesture Based Interfaces - Pervasive 2011</td> 
                        </tr>                         
                    </tbody>                     
                </table>                 
                <table width="484" border="0" align="center"> 
                    <tbody> 
                        <h3>Virtual and Augmented Reality</h3> 
                        <tr> 
                            <td> 
                                <img src="images/IEEEVR.jpg" alt="" width="75" height="75"/> 
                            </td>                             
                            <td> 
                                <img src="images/ISMAR_2019.jpg" alt="" width="75" height="75"/> 
                            </td>                             
                            <td> 
                                <img src="images/SIGGRAPH_2019.jpg" alt="" width="75" height="75"/> 
                            </td>                             
                            <td> 
                                <img src="images/SCA_2019.jpg" alt="" width="75" height="75"/> 
                            </td>                             
                            <td> 
                                <img src="images/vrst.jpg" alt="" width="75" height="75"/> 
                            </td>                             
                            <td> 
                                <img src="images/u4.jpeg" alt="" width="100" height="80"/> 
                            </td>                             
                        </tr>                         
                        <tr> 
                            <td>Real World Objects for Egocentric VR - IEEEVR 2020</td> 
                            <td>Creating Virtual Props - ISMAR 2019</td> 
                            <td>Real Time Object Deformation for VR - SIGGRAPH 2019 (poster)</td> 
                            <td>Multi-Camera / RGBD Object Tracking - SCA 2019</td> 
                            <td>Tracking Head Mounted Displays - VRST 2014</td> 
                            <td>Latency Aware Foveated Rendering - CVMP 2015</td> 
                        </tr>                         
                    </tbody>                     
                </table>                 <br> <br> 
                <div class="title"> 
                    <h2>Alumni</h2> <a name="team"></a> 
                </div>                 
                <div id="team"> 
                    <p>At Microsoft I lead a team of amazingly talented scientists and engineers. But as a Professor and previous Director of CAMERA I was also fortunate to work with some amazing students, researchers and engineers.</p>
                    <p> Martin Parsons (CAMERA),  Murray Evans (CAMERA), Yiguo Qiao (Living With/RUH/InnovateUK), Jack Saunders, George Fletcher, Jake Deane, Kyle Reed (Cubic Motion); Jose Serra (Digital Domain/ ILM), Anamaria Ciucanu (MMU), Pedro Mendes, Shridhar Ravikumar (Amazon, Apple); Alastair Barber (The Foundry); Wenbin Li (Bath); Han Gong (Apple); Charalampos Koniaris (Disney Research); Daniel Beale; Sinan Mutlu (Framestore); Nicholas Swafford; Nadejda Roubtsova (CAMERA); Sinead Kearney (CAMERA); Maryam Naghizadeh; Catherine Taylor (Marshmallow Laser Feast)</p> 
                    <bl> </bl>                     
                    <ul class="actions"> 
                        <li> <a href="#" class="button">Etiam posuere</a> 
                        </li>                         
                    </ul>                     
                </div>                 
                <div class="title"> 
                    <h2>Research Funding and Awards</h2> <a name="funding"></a> 
                </div>                 
                <div id="grants"> 
                    <p>(PI) 2020-2025: CAMERA 2.0 - Centre for the Analysis of Motion, Entertainment Research and Applications (£4,151,614 FEC). <strong>EPSRC</strong></p> 
                    <p>(PI) 2019-2021: CAMERA Motion Capture Innovation Studio (£901,391) <strong>Horizon 2020</strong></p> 
                    <p>(PI) 2019-2022: A tool to reveal Individual Differences in Facial Perception (£402,113) <strong>Medical Research Council (MRC) </strong></p> 
                    <p>(PI) 2018-2020: Rheumatoid Arthritis Flare Profiler (£165,126, Total project value £663,290). Partners: Living With, NHS. <strong>InnovateUK</strong></p> 
                    <p>(Co-I) 2018-2022: Bristol and Bath Creative Cluster (~£4m). Partners: UWE, University of Bristol, Bath Spa University. <strong>AHRC</strong></p> 
                    <p>(PI) 2017-2019: DOVE: Deformable Objects for Virtual Environments (£128,746, Total project value £562,559 FEC). Partner: Marshmallow Laser Feast, Heston's Fat Duck. <strong>Innovate UK</strong></p> 
                    <p>(PI) 2016-2018: HARPC: HMC for Augmented Reality Performance Capture (£119,025, Total project value £517,616 FEC). Partner: The Imaginarium. <strong>Innovate UK</strong></p> 
                    <p><p>(PI) 2015-2020: Centre for the Analysis of Motion, Entertainment Research and Applications - CAMERA (£ 4,998,728 FEC). Partners: The Imaginarium, The Foundry, Ministry of Defence, British Maratime Technologies, British Skeleton. <strong>EPSRC/AHRC</strong>. (not including partner contributions, ~£5,000,000).</p> <p>(PI) 2015-2017: Biped to Animal (£108,109 FEC). Parter: The Imaginarium.<strong> Innovate UK.</strong></p> <p>(PI) 2015: Goal Oriented Real Time Intelligent Performance Retargeting &nbsp;(£29,997 FEC). Partner: The Imaginarium. <strong>Innovate UK</strong>. </p> <p>(Co-I) 2013-2016: Acquiring Complete and Editable Outdoor Models from Video and Images (£1,003,256 FEC).<strong> EPSRC</strong>.</p> <p>(PI-Bath) 2014-2017: Visual Image Interpretation in Man and Machine (VIIMM) (£121,030 FEC). Partner: University of Birmingham. <strong>EPSRC</strong></p> <p>(PI) 2012-2016: Next Generation Facial Capture and Animation (£100,887 FEC). Partner: Double Negative Visual Effects. The <strong>Royal Society Industry Fellowship</strong>. </p> <p>(PI) 2007-2012: Exploiting 4D Data for Creating Next Generation Facial Modelling and Animation Techniques (£460,640FEC). <strong>The Royal Academy of Engineering Research Fellowship</strong>. </p> <p>Other funding: PhD Studentships, EPSRC Innovation Acceleration Account (IAA), Nuffield Foundation. </p> <bl> </bl>  
                </div>                 
                <div class="title"> 
                    <h2>Code and Data</h2> <a name="datasets"></a> 
                </div>                 
                <div id="data" style="text-align: left"> 
                    <p><strong>RGBD-Dog</strong> RGBD-Dog contains motion capture and multiview (Sony) RGB and (Kinect) RGBD data for several dogs performing different actions (all cameras and mo-cap syncronised with calibration data included. You can get the data, code to view and the CVPR 2020 paper it is all based on from our <a href="https://github.com/CAMERA-Bath/RGBD-Dog">GitHub page</a>. In our CVPR 2020 paper we use the data to train a model to predict dog pose from RGBD data. However, it also works pretty well on other animals. In the future we will expand the data and code as we publish more of our research. </p> 
                    <p><b>D3DFACS</b> The D3DFACS Dataset contains over 500 FACS coded dynamic 3D (4D) sequences from 10 individuals - including 3D meshes, stereo UV maps, colour camera images and calibration files. You can find out more about it in our ICCV 2011 paper "<a href="ICCV_final_2011.pdf">A FACS Valid 3D Dynamic Action Unit Database with Applications to 3D Dynamic Morphable Facial Modelling</a>". If you would like to download the dataset for academic research, <a href="https://vision.cs.bath.ac.uk/facedata">please visit the data set website</a></p> 
                    <p><b>Shadow
			Removal Ground Truth and Evaluation</b> To encourage the open comparison of single image
                  shadow removal in community, we provide an online
                  benchmark site and a dataset. Our quantitatively
                  verified high quality dataset contains a wide range of
                  ground truth data (214 test cases in total). Each case
                  is rated according to 4 attributes, which are texture,
                  brokenness, colourfulness and softness, in 3
                  perceptual degrees from weak to strong. To access the
                  evaluation website, <a href="http://www.cs.bath.ac.uk/%7Ehg299/shadow_eval/">please visit here</a>.</p> 
                    <p><strong>Other Code</strong> I'm trying to archive and add new code to my <a href="https://github.com/dopomoc">GitHub page</a> (under username dopomoc) when I get time - so please check that out now and again for updates!</p> <br> <br> 
                </div>                 <br> 
                <div id="papers"> 
                    <div class="title"> 
                        <h2>Publications (Recent and Selected)</h2> <a name="papers"></a> 
                    </div>                     
                    <p>There are many better systems these days at keeping track of personal papers - e.g. my <a href="https://scholar.google.co.uk/citations?user=n0rYM4kAAAAJ&hl=en">Google Scholar page</a> or <a href="https://researchportal.bath.ac.uk/en/persons/darren-cosker/publications/">University of Bath Pure page</a>. So, I apologies if this page is not maintained as well as I would like and papers are missing!</p> 
                    <ul class="style1"> 
                        <li> 
                            <p class="date"><img src="images/RGBDDog.jpg" alt="" width="75" height="50"/></p> 
                            <h3><a href="">RGBD-Dog: Predicting Canine Pose from RGBD Sensors</a></h3> 
                            <p>S. Kearney, W. Li, M. Parsons, K. I. Kim and D. Cosker</p> 
                            <p>IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), 2020</p> 
                        </li>                         
                        <li> 
                            <p class="date"><img src="images/CAMERA_C_logo.jpg" alt="" width="75" height="75"/></p> 
                            <h3><a href="https://purehost.bath.ac.uk/admin/files/204633157/USING_COMPUTER_VISION_AND_DEEP_LEARNING_METHODS_TO_CAPTURE_SKELET.pdf">Using Computer Vision and Deep Learning Methods to Capture Skeleton Push Start Performance</a></h3> 
                            <p>L. Needham, M. Evans, D. Cosker and S. Colyer</p> 
                            <p>International Conference on Biomechanics in Sport, 2020</p> 
                        </li>                         
                        <li> 
                            <p class="date"><img src="images/IEEEVR.jpg" alt="" width="75" height="75"/></p> 
                            <h3><a href="">Transporting Real World Rigid and Articulated Objects into Egocentric VR Experiences</a></h3> 
                            <p>C. Taylor, R. McNicholas and D. Cosker</p> 
                            <p>IEEE VR, 2020</p> 
                        </li>                         
                        <li> 
                            <p class="date"><img src="images/WEVR_2020.jpg" alt="" width="75" height="50"/></p> 
                            <h3><a href="">Towards An Egocentric Framework for Rigid and Articulated Object Tracking in Virtual Reality</a></h3> 
                            <p>KC. Taylor, R. McNicholas and D. Cosker</p> 
                            <p>IEEE VR Workshop on Everyday Virtual Reality (WEVR), 2020</p> 
                        </li>                         
                        <li> 
                            <p class="date"><img src="images/CGF_2019.jpg" alt="" width="75" height="50"/></p> 
                            <h3><a href="https://researchportal.bath.ac.uk/en/publications/user-guided-facial-animation-through-an-evolutionary-interface">User-Guided Facial Animation through an Evolutionary Interface</a></h3> 
                            <p>K. Reed and D. Cosker</p> 
                            <p>Computer Graphics Forum (presented at ACM Symposium on Computer Animation), 2019</p> 
                        </li>                         
                        <li> 
                            <p class="date"><img src="images/ISMAR_2019.jpg" alt="" width="75" height="50"/></p> 
                            <h3><a href="VRProps - ISMAR 2019.pdf">VR Props: An End-to-End Pipeline for Transporting Real Objects into Virtual and Augmented Environment</a></h3> 
                            <p>C. Taylor, C. Mullany, R. McNicholas and D. Cosker</p> 
                            <p>International Symposium on Mixed and Augmented Reality (ISMAR), 2019</p> 
                        </li>                         
                        <li> 
                            <p class="date"><img src="images/CGI_2019.jpg" alt="" width="75" height="50"/></p> 
                            <h3><a href="https://researchportal.bath.ac.uk/en/publications/multi-character-motion-retargeting-for-large-scale-transformation">Multi-character Motion Retargeting for Large-Scale Transformations</a></h3> 
                            <p>M. Naghizadeh and D. Cosker</p> 
                            <p>Computer Graphics International (CGI), 2019</p> 
                        </li>                         
                        <li> 
                            <p class="date"><img src="images/SIGGRAPH_2019.jpg" alt="" width="75" height="50"/></p> 
                            <h3><a href="VRProp-Net - Siggraph Final.pdf">VRProp-Net:Real-time Interaction with Virtual Props</a></h3> 
                            <p>C. Taylor, R. McNicholas and D. Cosker</p> 
                            <p>ACM SIGGRAPH (Poster), 2019</p> 
                        </li>                         
                        <li> 
                            <p class="date"><img src="images/SCA_2019.jpg" alt="" width="75" height="50"/></p> 
                            <h3><a href="SCA.pdf">Transporting Real Objects into Virtual and Augmented Environments</a></h3> 
                            <p>C. Taylor, M. Evans, R. McNicholas and D. Cosker</p> 
                            <p>ACM Symposium on Computer Animation (Sketch), 2019</p> 
                        </li>                         
                        <li> 
                            <p class="date"><img src="images/NIPS_2018.jpg" alt="" width="75" height="50"/></p> 
                            <h3><a href="https://arxiv.org/abs/1806.02311">Unsupervised Attention-guided Image-to-Image Translation</a></h3> 
                            <p>Y. Alami Mejjati, C. Richardt, J. Tompkin, D. Cosker and K. I. Kim </p> 
                            <p>Advances in Neural Information Processing Systems (NIPS), 2018</p> 
                        </li>                         
                        <li> 
                            <p class="date"><img src="images/MIG_2018.jpg" alt="" width="75" height="50"/></p> 
                            <h3><a href="http://www.yongliangyang.net/docs/estopmotion_mig18.pdf">E-StopMotion: Digitizing Stop Motion for Enhanced Animation and Games</a></h3> 
                            <p>A. Ciucana, N. Bhandari, X. Wu, S. Ravikumar, Y. Yang and D. Cosker</p> 
                            <p>ACM SIGGRAPH Conference on Motion Interaction and Games (MIG), 2018</p> 
                        </li>                         
                        <li> 
                            <p class="date"><img src="images/SPORT_2018.jpg" alt="" width="75" height="50"/></p> 
                            <h3><a href="https://researchportal.bath.ac.uk/en/publications/a-review-of-the-evolution-of-vision-based-motion-analysis-and-the">A Review of the Evolution of Vision-based Motion Analysis and the Integration of Advanced Computer Vision Methods towards Developing a Markerless System</a></h3> 
                            <p>S. Colyer, M. Evans, D. Cosker and A. Salo </p> 
                            <p>Sport Medicine, 2018</p> 
                        </li>                         
                        <li> 
                            <p class="date"><img src="images/FOOT_2018.jpg" alt="" width="75" height="50"/></p> 
                            <h3><a href="https://ieeexplore.ieee.org/document/8354288">Foot Contact Timings and Step Length for Sprint Training</a></h3> 
                            <p>M. Evans, S. Colyer, D. Cosker and A. Salo </p> 
                            <p>IEEE Winter Conference on Applications of Computer Vision (WACV), 2018</p> 
                        </li>                         
                        <li> 
                            <p class="date"><img src="images/CVPR_2018.jpg" alt="" width="75" height="50"/></p> 
                            <h3><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Mejjati_Multi-Task_Learning_by_CVPR_2018_paper.pdf">Multi-task Learning by Maximizing Statistical Dependence</a></h3> 
                            <p>Y. Alami Mejjati, D. Cosker and K. I. Kim </p> 
                            <p>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018</p> 
                        </li>                         
                        <li> 
                            <p class="date"><img src="images/MIG_2016.jpg" alt="" width="75" height="50"/></p> 
                            <h3><a href="https://researchportal.bath.ac.uk/en/publications/easy-generation-of-facial-animation-using-motion-graphs">Easy Generation of Facial Animation using Motion Graphs</a> </h3> 
                            <p>J. Serra, O Cetinaslan, S. Ravikumar, V. Orvalho and D. Cosker </p> 
                            <p>Computer Graphics Forum, 2018 (presented at EUROGRAPHICS 2018)</p> 
                        </li>                         
                        <li> 
                            <p class="date"><img src="images/blur_robust.jpg" alt="" width="75" height="50"/></p> 
                            <h3><a href="https://researchportal.bath.ac.uk/en/publications/learn-to-model-blurry-motion-via-directional-similarity-and-filte">Learn to Model Blurry Motion via Directional Similarity and Filtering</a></h3> 
                            <p>W. Li, D. Chen, L. Zhihan, Y. Yan and D. Cosker</p> 
                            <p>Pattern Recognition, 2018</p> 
                        </li>                         
                        <!--
			<li>
			    <p class="date"><img src="images/book.png" alt="" width="75" height="50" /></p>
			    <h3>Automatic Structual Scene Digitalization </h3>
			    <p>R. Tang, Y. Wang, D. Cosker and W. Li</p>
			    <p>PLoS ONE, 2017</p>
            </li>
-->                         
                        <li> 
                            <p class="date"><img src="images/uccv14.jpg" alt="" width="75" height="50"/></p> 
                            <h3><a href="https://ueaeprints.uea.ac.uk/63266/1/ivc_final.pdf">User-Assisted Image Shadow Removal</a></h3> 
                            <p>Han Gong, Darren Cosker</p> 
                            <p>Image and Vision Computing, 62, pp.19-27, 2017</p> 
                        </li>                         
                        <li> 
                            <p class="date"><img src="images/JOSA_2016.png" alt="" width="75" height="50"/></p> 
                            <h3><a href="https://arxiv.org/abs/1608.00762">Interactive Removal and Ground Truth for Difficult Shadow Scenes</a></h3> 
                            <p><a href="https://arxiv.org/abs/1608.00762">Han Gong, Darren Cosker</a></p> 
                            <p><a href="https://arxiv.org/abs/1608.00762">Journal of the Optical Society of America (JOSA) A, 2016</a></p> 
                        </li>                         
                        <li> 
                            <p class="date"><img src="images/MIG_2016.jpg" alt="" width="75" height="50"/></p> 
                            <h3><a href="papers/MIG_2016.pdf">Behavioural facial animation using motion graphs and mind maps</a></h3> 
                            <p><a href="papers/MIG_2016.pdf">Jose Serra, Darren Cosker, Veronica Orvalho</a></p> 
                            <p><a href="papers/MIG_2016.pdf">ACM SIGGRAPH Conference on Motion Interaction and Games (MIG), 2016</a></p> 
                        </li>                         
                        <li> 
                            <p class="date"><img src="images/RAL_2016.bmp" alt="" width="75" height="50"/></p> 
                            <h3><a href="papers/RAL_2016.pdf">Nonrigid Optical Flow Ground Truth for Real-World Scenes with Time-Varying Shading Effects</a></h3> 
                            <p><a href="papers/RAL_2016.pdf">Wenbin Li, Darren Cosker, Zhihan Lv, Matthew Brown</a></p> 
                            <p><a href="papers/RAL_2016.pdf">IEEE Robotics and Automation Letters (RA-L), 2016</a></p> 
                        </li>                         
                        <li> 
                            <p class="date"><img src="images/CASE_2016.bmp" alt="" width="75" height="50"/></p> 
                            <h3><a href="papers/RAL_2016.pdf">Dense Nonrigid Ground Truth for Optical Flow in Real-World Scenes</a></h3> 
                            <p><a href="papers/RAL_2016.pdf">Wenbin Li, Darren Cosker, Zhihan Lv, Matthew Brown</a></p> 
                            <p><a href="papers/RAL_2016.pdf">IEEE Conference on Automation Science and Engineering (CASE), 2016</a></p> 
                        </li>                         
                        <li> 
                            <p class="date"><img src="images/digipro.jpg" alt="" width="75" height="50"/></p> 
                            <h3><a href="papers/Digipro_2016.pdf">Camera Tracking in Visual Effects - An Industry Perspective of Structure from Motion</a></h3> 
                            <p><a href="papers/Digipro_2016.pdf">Alastair Barber, Darren Cosker, Oliver James, Ted Waine and Radhika Patel</a></p> 
                            <p><a href="papers/Digipro_2016.pdf">Digital Production Symposium (DigiPro) 2016</a></p> 
                        </li>                         
                        <li> 
                            <p class="date"><img src="images/sap_2016.jpg" alt="" width="75" height="50"/></p> 
                            <h3><a href="papers/SAP_2016.pdf">User, Metric and Computational Evaluation of Foveated Rendering Methods</a></h3> 
                            <p><a href="papers/SAP_2016.pdf"></a>Nicholas T. Swafford, Jose A. Iglesias-Guitian, Charalampos Koniaris, Bochang Moon, Darren Cosker and Kenny Mitchell</p> 
                            <p><a href="papers/SAP_2016.pdf">ACM Symposium on Applied Perception (SAP) 2016</a></p> 
                        </li>                         
                        <li> 
                            <p class="date"><img src="images/GI_2016.jpg" alt="" width="75" height="50"/></p> 
                            <h3><a href="papers/GI_2016.pdf">Reading Between the Dots: Combining 3D Markers and FACS Classiﬁcation for High-Quality Blendshape Facial Animation</a></h3> 
                            <p><a href="papers/GI_2016.pdf">Shridhar Ravikumar, Colin Davidson, Dimitri Kit, Neill Campbell, Luca Benedetti and Darren Cosker</a></p> 
                            <p><a href="papers/GI_2016.pdf">Graphics Interface (GI) 2016</a></p> 
                        </li>                         
                        <li> 
                            <p class="date"><img src="images/CVM.jpg" alt="" width="75" height="50"/></a> 
                        </p>                         
                        <h3><a href="papers/CVM_2016.pdf">Fitting Quadrics with a Bayesian Prior</a></h3> 
                        <p><a href="papers/CVM_2016.pdf">Daniel Beale, Yong-Liang Yang, Neill Campbell, Darren Cosker and Peter Hall</a></p> 
                        <p><a href="papers/CVM_2016.pdf">Computational Visual Media, 2016</a></p> 
                    </li>                     
                    <li> 
                        <p class="date"><img src="images/video_interp.jpg" alt="" width="75" height="50"/></a> 
                    </p>                     
                    <h3><a href="papers/Neurocomputing_Video_2016.pdf">Video Interpolation using Optical Flow 
 and Laplacian Smoothness</a></h3> 
                    <p><a href="papers/Neurocomputing_Video_2016.pdf">Wenbin Li, Darren Cosker</a></p> 
                    <p><a href="papers/Neurocomputing_Video_2016.pdf">Neurocomputing 2016</a></p> 
                </li>                 
                <li> 
                    <p class="date"><img src="images/blur_robust.jpg" alt="" width="75" height="50"/></a> 
                </p>                 
                <h3><a href="papers/Neurocomputing_Blur_2016.pdf">Blur Robust Optical Flow using Motion Channel</a></h3> 
                <p><a href="papers/Neurocomputing_Blur_2016.pdf">Wenbin Li, Yang Chen, JeeHang Lee, Gang Ren, Darren Cosker</a></p> 
                <p><a href="papers/Neurocomputing_Blur_2016.pdf">Neurocomputing 2016</a></p> 
            </li>             
            <li> 
                <p class="date"><img src="images/drift_robust.jpg" alt="" width="75" height="50"/></a> 
            </p>             
            <h3><a href="papers/JIFS_Drift_2016.pdf">Drift Robust Non-rigid Optical Flow Enhancement
 for Long Sequences</a></h3> 
            <p><a href="papers/JIFS_Drift_2016.pdf">Wenbin Li, Darren Cosker, Matthew Brown</a></p> 
            <p><a href="papers/JIFS_Drift_2016.pdf">Journal of Intelligent and Fuzzy Systems, JIFS 2016</a></p> 
        </li>         
        <li> 
            <p class="date"><img src="images/vfx.jpeg" alt="" width="75" height="50"/></a> 
        </p>         
        <h3><a href="papers/VFX_2015.pdf">Facial Capture and Animation in Visual Effects</a></h3> 
        <p><a href="papers/VFX_2015.pdf">Darren Cosker, Peter Eisert and Volker Helzle</a></p> 
        <p><a href="papers/VFX_2015.pdf">'Digital Representations of the Real World: How to Capture, Model, and Render Visual Reality', CRC Press. Marcus A. Magnor, Oliver Grau, Olga Sorkine-Hornung, Christian Theobalt (Eds.), 2015</a></p> 
    </li>     
    <li> 
        <p class="date"><img src="images/inferring.jpg" alt="" width="75" height="50"/></a> 
    </p>     
    <h3><a href="papers/CAG_2015.pdf">Inferring Changes in Intrinsic Parameters from Motion Blur</a></h3> 
    <p><a href="papers/CAG_2015.pdf">A Barber, M Brown, P Hogbin, D Cosker </a></p> 
    <p><a href="papers/CAG_2015.pdf">Computers &amp; Graphics, 52, 155-170, 2015</a></p> 
</li> 
<li> 
    <p class="date"><img src="images/u4.jpeg" alt="" width="75" height="50"/></a> 
</p> 
<h3><a href="papers/CVMP_Nick_2015.pdf">Latency aware foveated rendering in unreal engine 4</a> </h3> 
<p><a href="papers/CVMP_Nick_2015.pdf">NT Swafford, D Cosker, K Mitchell </a></p> 
<p><a href="papers/CVMP_Nick_2015.pdf">Proceedings of the 12th European Conference on Visual Media Production (CVMP), 2015</a></p> 
<li> 
    <p class="date"><img src="images/rigidity.jpg" alt="" width="75" height="50"/></a> 
</p> 
<h3><a href="papers/CVMP_Babis_2015.pdf">Real-time variable rigidity texture mapping</a></h3> 
<p><a href="papers/CVMP_Babis_2015.pdf">C Koniaris, K Mitchell, D Cosker </a></p> 
<p><a href="papers/CVMP_Babis_2015.pdf">Proceedings of the 12th European Conference on Visual Media Production (CVMP), 2015</a></p> 
<li> 
    <p class="date"><img src="images/FAA_2015.jpg" alt="" width="75" height="50"/></a> 
</p> 
<h3><a href="papers/FAA_2015.pdf">Perceived Emotionality of Linear and Non-Linear AUs Synthesised using a 3D Dynamic Morphable Facial Model</a></h3> 
<p><a href="papers/FAA_2015.pdf">D Cosker, E Krumhuber, A Hilton</a></p> 
<p><a href="papers/FAA_2015.pdf">Facial Analysis and Animation/Audio Visual Speech Processing (FAAVSP), 2015</a></p> 
<li> 
    <p class="date"><img src="images/blur.jpg" alt="" width="75" height="50"/></a> 
</p> 
<h3><a href="papers/CVMP_Alastair_2014.pdf">Estimating camera intrinsics from motion blur</a></h3> 
<p><a href="papers/CVMP_Alastair_2014.pdf">A Barber, M Brown, P Hogbin, D Cosker </a></p> 
<p><a href="papers/CVMP_Alastair_2014.pdf">Proceedings of the 11th European Conference on Visual Media Production (CVMP), 2014</a> </p> 
<li> 
    <p class="date"><img src="images/vrst.jpg" alt="" width="75" height="50"/></a> 
</p> 
<h3><a href="papers/VRST_2014.pdf">Dual sensor filtering for robust tracking of head-mounted displays</a></h3> 
<p><a href="papers/VRST_2014.pdf">Nicholas T Swafford, Bastiaan J Boom, Kartic Subr, David Sinclair, Darren Cosker, Kenny Mitchell </a></p> 
<p><a href="papers/VRST_2014.pdf">ACM Symp. on VR Software and Technology (VRST), 2014</a></p> 
<li> 
    <p class="date"><img src="images/uccv14.jpg" alt="" width="75" height="50"/></a> 
</p> 
<h3><a href="papers/ACCV_2014.pdf">Interactive Shadow Editing from Single Images</a></h3> 
<p><a href="papers/ACCV_2014.pdf">H Gong, D Cosker </a></p> 
<p><a href="papers/ACCV_2014.pdf">Asian Conference on Computer Vision (ACCV), Workshops, 243-252, 2014</a></p> 
<li> 
    <p class="date"><img src="images/bmvc14.jpg" alt="" width="75" height="50"/></a> 
</p> 
<h3><a href="papers/BMVC_2014.pdf">Interactive shadow removal and ground truth for variable scene categories (<strong>BEST STUDENT PAPER PRIZE</strong>)</a></h3> 
<p><a href="papers/BMVC_2014.pdf">H Gong, D Cosker </a></p> 
<p><a href="papers/BMVC_2014.pdf">British Machine Vision Conference (BMVC), 2014</a></p> 
<li> 
    <p class="date"><img src="images/surveyJCGT.jpg" alt="" width="75" height="50"/></a> 
</p> 
<h3><a href="papers/JCGT_2014.pdf">Survey of Texture Mapping Techniques for Representing and Rendering Volumetric Mesostructure</a></h3> 
<p><a href="papers/JCGT_2014.pdf">C Koniaris, D Cosker, X Yang, K Mitchell </a></p> 
<p><a href="papers/JCGT_2014.pdf">Journal of Computer Graphics Techniques, 2014</a></p> 
<li> 
    <p class="date"><img src="images/WACV_2013.jpg" alt="" width="75" height="50"/></a> 
</p> 
<h3><a href="papers/WACV_2014.pdf">Robust optical flow estimation for continuous blurred scenes using rgb-motion imaging and directional filtering <strong>(BEST STUDENT PAPER PRIZE)</strong></a></h3> 
<p><a href="papers/WACV_2014.pdf">W Li, Y Chen, JH Lee, G Ren, D Cosker </a></p> 
<p><a href="papers/WACV_2014.pdf">IEEE Winter Conf. on Applications of Comp. Vis. (WACV),792-799, 2014</a></p> 
<li> 
    <p class="date"><img src="images/cvmp13.png" alt="" width="75" height="50"/></a> 
</p> 
<h3><a href="papers/CVMP_2013_Babis.pdf">Real-time content-aware texturing for deformable surfaces</a></h3> 
<p><a href="papers/CVMP_2013_Babis.pdf">C Koniaris, D Cosker, X Yang, K Mitchell, I Matthews </a></p> 
<p><a href="papers/CVMP_2013_Babis.pdf">Proceedings of the 10th European Conference on Visual Media Production (CVMP), 2013</a></p> 
<li> 
    <p class="date"><img src="images/MM.jpg" alt="" width="75" height="50"/></a> 
</p> 
<h3><a href="papers/MM_2013.pdf">Applications of face analysis and modeling in media production</a></h3> 
<p><a href="papers/MM_2013.pdf">D Cosker, P Eisert, O Grau, PJB Hancock, J McKinnell, EJ Ong </a></p> 
<p><a href="papers/MM_2013.pdf">IEEE Multimedia, 20 (4), 18-27, 2013</a></p> 
<li> 
    <p class="date"><img src="images/shadow_2.jpg" alt="" width="75" height="50"/></a> 
</p> 
<h3><a href="papers/ICME_2013.pdf">User-aided single image shadow removal</a></h3> 
<p><a href="papers/ICME_2013.pdf">H Gong, D Cosker, C Li, M Brown </a></p> 
<p><a href="papers/ICME_2013.pdf">IEEE International Conference on Multimedia and Expo (ICME), 2013 </a></p> 
<li> 
    <p class="date"><img src="images/TVCG_Water.jpg" alt="" width="75" height="50"/></a> 
</p> 
<h3><a href="papers/TVCG_2013.pdf">Water surface modeling from a single viewpoint video</a></h3> 
<p><a href="papers/TVCG_2013.pdf">C Li, D Pickup, T Saunders, D Cosker, D Marshall, PS Hall, P Willis </a></p> 
<p><a href="papers/TVCG_2013.pdf">IEEE Transactions onVisualization and Computer Graphics, &nbsp;19 (7), 1242-1251, 2013</a></p> 
<li> 
    <p class="date"><img src="images/LME_Faces.jpg" alt="" width="75" height="50"/></a> 
</p> 
<h3><a href="papers/CVPR_2013.pdf">Optical flow estimation using laplacian mesh energy</a></h3> 
<p><a href="papers/CVPR_2013.pdf">W Li, D Cosker, M Brown, R Tang </a></p> 
<p><a href="papers/CVPR_2013.pdf">IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2013 </a></p> 
<li> 
    <p class="date"><img src="images/rigidity.jpg" alt="" width="75" height="50"/></a> 
</p> 
<h3>Content aware texture mapping on deformable surfaces</h3> 
<p><a href="#">C Koniaris, D Cosker, X Yang, KJ Mitchell, I Matthews </a></p> 
<p>US Patent App. 13/838,840 </p> 
<li> 
    <p class="date"><img src="images/anchor_patch.jpg" alt="" width="75" height="50"/></a> 
</p> 
<h3><a href="papers/ACCV_2013.pdf">An anchor patch based optimization framework for reducing optical flow drift in long image sequences</a></h3> 
<p><a href="papers/ACCV_2013.pdf">W Li, D Cosker, M Brown </a></p> 
<p><a href="papers/ACCV_2013.pdf">Asian Conference on Computer Vision (ACCV), 112-125, 2013</a></p> 
<li> 
    <p class="date"><img src="images/cvmp_water.jpg" alt="" width="75" height="50"/></a> 
</p> 
<h3><a href="papers/CVMP_2011.pdf">Realtime video based water surface approximation</a></h3> 
<p><a href="papers/CVMP_2011.pdf">C Li, M Shaw, D Pickup, D Cosker, P Willis, P Hall </a></p> 
<p><a href="papers/CVMP_2011.pdf">European Conference in Visual Media Production (CVMP), 109-117, 2011 </a><br/> </p> 
<li> 
    <p class="date"><img src="images/iccv.jpg" alt="" width="75" height="50"/></a> 
</p> 
<h3><a href="papers/ICCV_2011.pdf">A FACS valid 3D dynamic action unit database with applications to 3D dynamic morphable facial modeling</a></h3> 
<p><a href="papers/ICCV_2011.pdf">D Cosker, E Krumhuber, A Hilton </a></p> 
<p><a href="papers/ICCV_2011.pdf">IEEE International Conference on Computer Vision (ICCV), 2296-2303, 2011. </a></p> 
<li> 
    <p class="date"><img src="images/hci.jpg" alt="" width="75" height="50"/></a> 
</p> 
<h3>Identifying and evaluating gestural interaction in ubiquitous and pervasive computing</h3> 
<p>Michael Wright, CJ Lin, Darren Cosker, Eamonn O'Neill, Paul Johnson </p> 
<p>Proceedings GW2011: The 9th International Gesture Workshop: Gesture in Embodied Communication and Human-Computer Interaction, 2011</p> 
<li> 
    <p class="date"><img src="images/sfs10.jpg" alt="" width="75" height="50"/></a> 
</p> 
<h3><a href="papers/ACCV_2011.pdf">Reconstructing mass-conserved water surfaces using shape from shading and optical flow</a></h3> 
<p><a href="papers/ACCV_2011.pdf">D Pickup, C Li, D Cosker, P Hall, P Willis </a></p> 
<p><a href="papers/ACCV_2011.pdf">Asian Conference on Computer Vision (ACCV), 189-201, 2011</a></p> 
<li> 
    <p class="date"><img src="images/hci.jpg" alt="" width="75" height="50"/></a> 
</p> 
<h3><a href="papers/Pervasive_2011.pdf">3D gesture recognition: an evaluation of user and system performance</a></h3> 
<p><a href="papers/Pervasive_2011.pdf">M Wright, CJ Lin, E O’Neill, D Cosker, P Johnson </a></p> 
<p><a href="papers/Pervasive_2011.pdf">Pervasive Computing. Lecture Notes in Computer Science (LNCS), 294-313, 2011</a></p> 
<li> 
    <p class="date"><img src="images/APGV_2010.jpg" alt="" width="75" height="50"/></a> 
</p> 
<h3><a href="papers/FAA_2010.pdf">A FACS validated 3D human facial model</a></h3> 
<p><a href="papers/FAA_2010.pdf">D Cosker, E Krumhuber, A Hilton </a></p> 
<p><a href="papers/FAA_2010.pdf">Proceedings of the SSPNET 2nd International Symposium on Facial Analysis and Animation (FAA), 2010</a></p> 
<li> 
    <p class="date"><img src="images/APGV_2010.jpg" alt="" width="75" height="50"/></a> 
</p> 
<h3><a href="papers/APGV_2010.pdf">Perception of linear and nonlinear motion properties using a FACS validated 3D facial model</a></h3> 
<p><a href="papers/APGV_2010.pdf">D Cosker, E Krumhuber, A Hilton </a></p> 
<p><a href="papers/APGV_2010.pdf">Proceedings of the 7th Symposium on Applied Perception in Graphics and Visualisation (APGV), 2010</a></p> 
<li> 
    <p class="date"><img src="images/facebio.jpg" alt="" width="75" height="50"/></a> 
</p> 
<h3><a href="http://onlinelibrary.wiley.com/doi/10.1002/9781118786642.ch17/summary">Facial Actions for Biometric Applications</a></h3> 
<p><a href="http://onlinelibrary.wiley.com/doi/10.1002/9781118786642.ch17/summary">Lanthao Benedikt, Paul L. Rosin, David Marshall, Darren Cosker, Hashmat Popat, Stephen Richmond</a></p> 
<p><a href="http://onlinelibrary.wiley.com/doi/10.1002/9781118786642.ch17/summary">'3D Imaging for Orthodontics and Maxillofacial Surgery', Eds. Chung How Kau, Stephen Richmond, 267-285, Wiley-Blackwell, 2010</a></p> 
<li> 
    <p class="date"><img src="images/Vedran_IEEE.jpg" alt="" width="75" height="50"/></a> 
</p> 
<h3><a href="papers/IEEE_SMA_2010.pdf">Assessing the uniqueness and permanence of facial actions for use in biometric applications</a></h3> 
<p><a href="papers/IEEE_SMA_2010.pdf">Lanthao Benedikt, Darren Cosker, Paul L Rosin, David Marshall </a></p> 
<p><a href="papers/IEEE_SMA_2010.pdf"> IEEE Transactions on Systems, Man and Cybernetics, Part A: Systems and Humans, Volume 40, Issue 3, 449-460, 2010</a></p> 
<li> 
    <p class="date"><img src="images/interview_face_images.jpg" alt="" width="75" height="50"/></a> 
</p> 
<h3><a href="papers/JNVB_2009.pdf">Effects of dynamic attributes of smiles in human and synthetic faces: A simulated job interview setting</a></h3> 
<p><a href="papers/JNVB_2009.pdf">E Krumhuber, ASR Manstead, D Cosker, D Marshall, PL Rosin </a></p> 
<p><a href="papers/JNVB_2009.pdf">Journal of Nonverbal Behavior 33 (1), 1-15, 2009</a></p> 
<li> 
    <p class="date"><img src="images/cyrill_BMVC.jpg" alt="" width="75" height="50"/></a> 
</p> 
<h3><a href="papers/BMVC_2009.pdf">Incremental Learning of Dynamical Models of Faces</a></h3> 
<p><a href="papers/BMVC_2009.pdf">C. Charron, Y. A. Hicks, P. Hall and D. Cosker </a></p> 
<p><a href="papers/BMVC_2009.pdf">In Proc. British Machine Vision Conference (BMVC), 2009.</a></p> 
<li> 
    <p class="date"><img src="images/CASA.jpg" alt="" width="75" height="50"/></a> 
</p> 
<h3><a href="papers/CASA_2009_Long.pdf">Laughing, crying, sneezing and yawning: Automatic voice driven animation of non-speech articulations</a></h3> 
<p><a href="papers/CASA_2009_Long.pdf">D Cosker, J Edge </a></p> 
<p><a href="papers/CASA_2009_Long.pdf">Proceedings of Computer Animation and Social Agents (CASA), 2009</a> </p> 
<li> 
    <p class="date"><img src="images/Vedran_IEEE.jpg" alt="" width="75" height="50"/></a> 
</p> 
<h3><a href="papers/Ortho_2008.pdf">Three‐dimensional motion analysis–an exploratory study. Part 1: Assessment of facial movement</a></h3> 
<p><a href="papers/Ortho_2008.pdf">H Popat, S Richmond, R Playle, D Marshall, PL Rosin, D Cosker </a></p> 
<p><a href="papers/Ortho_2008.pdf">Orthodontics &amp; craniofacial research 11 (4), 216-223, 2008</a></p> 
<li> 
    <p class="date"><img src="images/BTAS.jpg" alt="" width="75" height="50"/></a> 
</p> 
<h3><a href="papers/BTAS_2008.pdf">3D Facial Gestures in Biometrics: from Feasibility Study to Application</a></h3> 
<p><a href="papers/BTAS_2008.pdf">L. Benedikt, D. Cosker, D. Marshall, P. L. Rosin</a></p> 
<p><a href="papers/BTAS_2008.pdf">In Proc. of IEEE International Conference on Biometrics Theory, Applications and Systems (BTAS), pp 1-6, 2008</a></p> 
<li> 
    <p class="date"><img src="images/dynamic_ID.jpg" alt="" width="75" height="50"/></a> 
</p> 
<h3><a href="papers/BMVC_2008.pdf">Facial Dynamics in Biometric Identification</a></h3> 
<p><a href="papers/BMVC_2008.pdf">L Benedikt, V Kajic, D Cosker, PL Rosin, AD Marshall </a></p> 
<p><a href="papers/BMVC_2008.pdf">In Proc. of British Machine Vision Conference (BMVC), 1-10, 2008</a></p> 
<li> 
    <p class="date"><img src="images/Gav_Eva_Poser.jpg" alt="" width="75" height="50"/></a> 
</p> 
<h3><a href="papers/LNCS_2007.pdf">Re-mapping animation parameters between multiple types of facial model</a></h3> 
<p><a href="papers/LNCS_2007.pdf">D Cosker, R Borkett, D Marshall, PL Rosin</a></p> 
<p><a href="papers/LNCS_2007.pdf">Lecture Notes in Computer Science (LNCS), 4418, 365-378, 2007</a></p> 
<li> 
    <p class="date"><img src="images/dynamic_ID.jpg" alt="" width="75" height="50"/></a> 
</p> 
<h3><a href="papers/CVMP_2007.pdf">Construction and perceptual evaluation of a 3D head model</a></h3> 
<p><a href="papers/CVMP_2007.pdf">L Benedikt, E Krumhuber, A Calvert, D Cosker, PL Rosin, D Marshall European Conference on Visual Media Production (CVMP), 2007. </a></p> 
<li> 
    <p class="date"><img src="images/smile_traj_.jpg" alt="" width="75" height="50"/></a> 
</p> 
<h3><a href="papers/Emotion_2007.pdf">Facial dynamics as indicators of trustworthiness and cooperative behavior</a></h3> 
<p><a href="papers/Emotion_2007.pdf">E Krumhuber, ASR Manstead, D Cosker, D Marshall, PL Rosin, A Kappas </a></p> 
<p><a href="papers/Emotion_2007.pdf">Emotion 7 (4), 730, 2007</a></p> 
<li> 
    <p class="date"><img src="images/LanSCA.png" alt="" width="75" height="50"/></a> 
</p> 
<h3><a href="papers/SIGGRAPH_2007.pdf">Using Dynamic 3D Facial Data to Create 3D Appearance Models of Facial Action Units</a></h3> 
<p><a href="papers/SIGGRAPH_2007.pdf">L. Benedikt, E. Krumhuber, A. Calvert, D. Cosker, P. Rosin, D. Marshall ACM SIGGRAPH (poster), San Diego, 2007</a></p> 
<li> 
    <p class="date"><img src="images/CASA.jpg" alt="" width="75" height="50"/></p> 
    <h3><a href="papers/CVMP_2007.pdf">Automatic audio driven animation of non-verbal actions</a></h3> 
    <p><a href="papers/CVMP_2007.pdf">D Cosker, C Holt, G Whatling, PL Rosin</a></p> 
    <p><a href="papers/CVMP_2007.pdf">Conference on Visual Media Production (CVMP), 2007 </a></p> 
</li> 
<li> 
    <p class="date"><img src="images/Push.jpg" alt="" width="75" height="50"/></p> 
    <h3><a href="papers/ICSP_2006.pdf">Virtual friend: tracking and generating natural interactive behaviours in real video</a></h3> 
    <p><a href="papers/ICSP_2006.pdf">Yue Zheng, Yulia Hicks, Dave Marshall, Darren Cosker</a></p> 
    <p><a href="papers/ICSP_2006.pdf">8th IEEE International Conference on&nbsp;Signal Processing, 2006</a></p> 
</li> 
<li> 
    <p class="date"><img src="images/Thesis_Im.jpg" alt="" width="75" height="50"/></p> 
    <h3><a href="papers/PhD_2006.pdf">Animation of a Hierarchical Appearance Based Facial Model and Perceptual Analysis of Visual Speech</a></h3> 
    <p><a href="papers/PhD_2006.pdf">Darren Cosker</a></p> 
    <p><a href="papers/PhD_2006.pdf">PhD Thesis, Cardiff University, 2006</a></p> 
</li> 
<li> 
    <p class="date"><img src="images/McGurk.png" alt="" width="75" height="50"/></p> 
    <h3><a href="papers/TAP_2005.pdf">Toward perceptually realistic talking heads: Models, methods, and mcgurk</a></h3> 
    <p><a href="papers/TAP_2005.pdf">D Cosker, S Paddock, D Marshall, PL Rosin, S Rushton </a></p> 
    <p><a href="papers/TAP_2005.pdf">ACM Transactions on Applied Perception (TAP) 2 (3), 270-285, 2005</a></p> 
</li> 
<li> 
    <p class="date"><img src="images/BSS.jpg" alt="" width="75" height="50"/></p> 
    <h3><a href="papers/ICASSP_2005.pdf">Video assisted speech source separation</a></h3> 
    <p><a href="papers/ICASSP_2005.pdf">W Wang, D Cosker, Y Hicks, S Sanei, J Chambers </a></p> 
    <p><a href="papers/ICASSP_2005.pdf">IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2005</a></p> 
</li> 
<li> 
    <p class="date"><img src="images/VIS.jpg" alt="" width="75" height="50"/></p> 
    <h3><a href="papers/VIS_2004.pdf">Speech-driven facial animation using a hierarchical model</a></h3> 
    <p><a href="papers/VIS_2004.pdf">DP Cosker, AD Marshall, PL Rosin, YA Hicks Vision, </a></p> 
    <p><a href="papers/VIS_2004.pdf">IEE Proc. on Vision, Image and Signal Processing, 151 (4), 314-321, 2004</a></p> 
</li> 
<li> 
    <p class="date"><img src="images/Trellis.jpg" alt="" width="75" height="50"/></p> 
    <h3><a href="papers/ICPR_2004.pdf">Speech driven facial animation using a hidden markov coarticulation model</a></h3> 
    <p><a href="papers/ICPR_2004.pdf">D Cosker, D Marshall, PL Rosin, Y Hicks Pattern Recognition </a></p> 
    <p><a href="papers/ICPR_2004.pdf">17th IEEE International Conference on Pattern Recognition (ICPR), 2004</a></p> 
</li> 
<div class="title"> 
    <h2>Archive</h2> <a name="archive"></a> 
</div> 
<p>Below is a collection of previous activities - including workshops (at CVPR, ACCV, etc) and EPSRC research networks I have co-founded - kept here for future reference (mine as much as anything!)</p> <br> 
<h3>EPSRC Network on Visual Image Interpretation in Humans and Machines (ViiHM)</h3> 
<p> 
Understanding the environment via the sense of vision represents a challenging problem in computer science. Yet biological vision, as evidenced in the human visual system, seems to process the visual environment effortlessly. This supports the notion that understanding biological vision will help to solve problems in machine vision. However, some of the biggest advances in our understanding of human vision have occurred as a direct result of modern computing techniques. We can only really say we understand a complex system fully when we can recreate or simulate it, test hypotheses on the simulation, and take the simulation to the limits of its validity. 
The aims of the EPSRC VIIHM Network are: 
1.       To foster communication and joint projects between relevant research groups including those working on biological vision (human and non-human animals) computer vision and machine vision.
2.       To establish a series of grand challenges focused around well specified tasks where cross-over studies have a strong potential to provide robust solutions.
3.       To foster joint cross-discipline grant applications.
4.       To explore mechanisms to improve the utility of joint publications for both partners.
5.       To equip individual PhD and post-doctoral scientists to be future leaders of cross-over research projects.
6.       To establish a lasting vehicle for supporting cross-over biological and machine vision projects.
7.       To increase public engagement with the concept of biologically inspired computer vision. 
You can join and register for our first workshop at the same time or just join the Network here: http://www.viihm.org.uk</p> <br> 
<h3>EPSRC Network on Vision and Language (V&L Net)</h3> 
<p> 
The EPSRC Network on Vision and Language (V&L Net) is a forum for researchers from the fields of Computer Vision and Language Processing to meet, exchange ideas, expertise and technology, and form new partner- ships. Our aim is to create a lasting interdisciplinary research community situated at the language-vision interface, jointly working towards solutions for some of today's most challenging computational challenges, including image and video search, description of visual content and text-to-image generation.  
As a research collaboration forum, V&L Net has a real-life and a virtual dimension. We hold annual V&L Net meetings which combine the characteristics of an academic conference, a networking event and an exhibition. At the same time, the V&L Net website offers a wide variety of different tools and resources including networking tools and repositories of publications, data resources and software tools <p> <br> <h3>2nd Meeting of the EPSRC Network on Visual Image Interpretation in Humans and Machines (ViiHM), July 1st/2nd, 2015</h3> <p>We invite all academics and relevant industrial practitioners interested in the fostering of human and computer vision research to the first annual meeting of the EPSRC VIIHM Network. The annual meeting will focus on community building and will comprise of plenary talks from internationally renowned human and computer vision researchers, networking and community building opportunities and poster sessions. 
Full workshop details may be found here http://www.viihm.org.uk/home/events/second-workshop/</p> <br> <h3>2nd Workshop on User Centric Computer Vision (UCCV), 2014</h3> <p> 
UCCV 2014 is a workshop dedicated to research on interactive computer vision and methods for making computer vision more accessible to wider audiences. The workshop welcomes work on case studies, end-user applications, developer-centred approaches and many other aspects of computer vision.  
1st Meeting of the EPSRC Network on Visual Image Interpretation in Humans and Machines (ViiHM), September 24th-25th, 2014 
We invite all academics and relevant industrial practitioners interested in the fostering of human and computer vision research to the first annual meeting of the EPSRC VIIHM Network. The annual meeting will focus on community building and will comprise of plenary talks from internationally renowned human and computer vision researchers, networking and community building opportunities and poster sessions. 
Up to 80 applicants will then be invited to attend the workshop - based on a balance of early, mid and advanced career researchers. We will also aim to balance the mix of disciplines.
The meeting duration is from midday on the 24th to the afternoon of 25th September. Those interested should complete the form below and send it to the Network Administrator by 30th June 2014. Workshop applicants may optionally request a poster presentation using the same form. Posters may represent new work, a review of past work, an outline of planned work or position piece, or an outline of collaboration interests and opportunities. 
Full workshop details may be found here http://viihm.org.uk/workshop.html</p> <br> <h3>3rd Workshop On Vision And Language 2014 (VL'14), Dublin, 23rd August 2014 </h3> <p> 
Fragments of natural language, in the form of tags, captions, subtitles, surrounding text or audio, can aid the interpretation of image and video data by adding context or disambiguating visual appearance. In addition, labelled images are essential for training object or activity classifiers. On the other hand, visual data can help resolve challenges in language processing such as word sense disambiguation. Studying language and vision together can also provide new insight into cognition and universal representations of knowledge and meaning. Meanwhile, sign language and gestures are languages that require visual interpretation.  
We welcome papers describing original research combining language and vision. To encourage the sharing of novel and emerging ideas we also welcome papers describing new datasets, grand challenges, open problems, benchmarks and work in progress as well as survey papers. 
Full workshop details may be found here https://vision.cs.bath.ac.uk/VL_2014/ </p> <br> <h3>EPSRC Workshop on Vision and Language (2010-2013)</h3> <p> 
The EPSRC Network on Vision and Language (V&L Net) is a forum for researchers from the fields of Computer Vision and Language Processing to meet, exchange ideas, expertise and technology, and form new partner- ships. Our aim is to create a lasting interdisciplinary research community situated at the language-vision interface, jointly working towards solutions for some of today's most challenging computational challenges, including image and video search, description of visual content and text-to-image generation.  
As a research collaboration forum, V&L Net has a real-life and a virtual dimension. We hold annual V&L Net meetings which combine the characteristics of an academic conference, a networking event and an exhibition. At the same time, the V&L Net website offers a wide variety of different tools and resources including networking tools and repositories of publications, data resources and software tools. The networks home page may be found here. </p> <br> <h3>Eurographics UK - Theory and Practice of Computer Graphics 2013</h3> <p> 
The 31st Conference organised by the UK chapter of the Eurographics Association took place at the University of Bath on the 5-6 September 2013. The aim of this conference is to focus on theoretical and practical aspects of Computer Graphics and to bring together top practitioners, users and researchers, which will inspire further collaboration between participants particularly between academia and industry.  
The meeting website contains more details of the event.</p> <h3>IEEE CVPR Workshop on Vision and Language 2013</h3> <p> 
The EPSRC Network on Vision and Language (V&L Net) has been set up to foster collaborative work in this area. It is a forum for researchers from the fields of Computer Vision and Natural Language Processing to meet, exchange ideas, expertise and technology, and form new partnerships. The aim is to create a lasting interdisciplinary research community situated at the language-vision interface, jointly working towards solutions for some of today's most challenging computational challenges, including image and video search, description of visual content and text-to-image generation. A workshop on this theme - held jointly with CVPR - took place in 2013. 
The meeting website may be found here.</p> <br> <h3>Symposium on Facial Analysis and Animation (FAA), in Co-op with ACM, (2009, 2010, 2012)</h3> <p> 
The aim of this meeting is to bring together researchers and practitioners from both academia and industry – particularly in VFX and games - interested in all aspects of facial animation and related analysis. The meeting has previously been held in Edinburgh (2009/2010) and Vienna (2012). Watch this space for future meetings! </p> <h3>AVA/BMVA Biological and Computer Vision (2011,2012)</h3> <p> 
The study of biological and machine vision share much common history (e.g. Marr), and each discipline has benefited enormously from findings and techniques from the other.  In the UK (in contrast to elsewhere) the discussion and collaboration between the sister disciplines seems to have declined. The aim of this meeting, organised jointly by the Applied Vision Association (AVA) (UK biological vision) and British Machine Vision Association (BMVA) (UK computer vision) is to reignite conversations between these two fields. 
The meeting was held at Cardiff University (2011) and at Microsoft Research, Cambridge (2012). Watch this space for 2013 meeting information</p> <div id="copyright"> <span>&copy; Darren Cosker. All rights reserved. </span> <span>Design by <a href="http://templated.co" rel="nofollow">TEMPLATED</a>.</span> 
        </div> </div> </div>
