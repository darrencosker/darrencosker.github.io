<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"> 
<!--
Design by TEMPLATED
http://templated.co
Released for free under the Creative Commons Attribution License

Name       : Skeleton 
Description: A two-column, fixed-width design with dark color scheme.
Version    : 1.0
Released   : 20130902

--> 
<html xmlns="http://www.w3.org/1999/xhtml"> 
    <head> 
        <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/> 
        <title>Darren Cosker's Home Page</title>         
        <meta name="keywords" content=""/> 
        <meta name="description" content=""/> 
        <link href="../css/theme.css" rel="stylesheet" type="text/css"> 
        <link href="http://fonts.googleapis.com/css?family=Source+Sans+Pro:200,300,400,600,700,900" rel="stylesheet"/> 
        <link href="default.css" rel="stylesheet" type="text/css" media="all"/> 
        <link href="fonts.css" rel="stylesheet" type="text/css" media="all"/> 
        <!--[if IE 6]><link href="default_ie6.css" rel="stylesheet" type="text/css" /><![endif]-->         
    </head>     
    <body> 
        <div id="page" class="container"> 
            <div id="header"> 
                <div id="logo"> 
                    <img src="images/darren_6.jpg" alt="" width="165" height="165"/> 
                    <h2><a href="#">Prof. Darren Cosker</a></h2> 
                    <p><font color="white"><b>Principal Scientist Manager</b> @ Microsoft</font></p> 
                    <p><font color="white"><b>Professor</b> @ University of Bath/Royal Society Industry Fellow</font></p> <span></span> 
                    <p><font color="white">Royal Society Industry Fellow</font></p> <span></span> 
                    <p><font color="white">Contact: <a href="https://www.linkedin.com/in/darren-cosker-61271413/" LinkedIn< a> or <a href="https://twitter.com/dopomoc">X/Twitter</a></font></p> 
                </div>                 
                <div id="menu"> 
                    <ul> 
                        <li> <a href="http://www.camera.ac.uk">CAMERA</a> 
                        </li>                         
                        <li> <a href="#areas">RESEARCH AREAS</a> 
                        </li>                         
                        <li> <a href="#funding">FUNDING</a> 
                        </li>                         
                        <li> <a href="#team">TEAM</a> 
                        </li>                         
                        <li> <a href="#papers">PAPERS</a> 
                        </li>                         
                        <li> <a href="#commercial">COMMERCIAL</a> 
                        </li>                         
                        <li> <a href="#datasets">CODE+DATA</a> 
                        </li>                         
                        <li> <a href="#archive">ARCHIVE</a> 
                        </li>                         
                    </ul>                     
                </div>                 
            </div>             
            <div id="main" align="center"> 
                <div id="banner"> 
                    <img src="images/banner_v2.jpg" alt="" class="image-full"/> 
                </div>                 
                <!--
		<table width="200" border="0" align="center" cellpadding="10">
  <tbody>
    <tr>
      <td><img src="images/darren_6.jpg" alt="" width="150" height="150" /></td>
      <td>Prof. Darren Cosker
          <p>D.P.Cosker@bath.ac.uk</p>
          <p>Professor/Royal Society Industry Fellow</p></td>
    </tr>
  </tbody>
</table>
-->                 <a href="http://www.camera.ac.uk"><img src="images/CAMERA_C_logo.jpg" width="55" height="50" width="50" height="50" alt=""/></a> <a href="https://github.com/dopomoc"><img src="images/github_2.png" width="50" height="50"></a> <a href="https://twitter.com/dopomoc"><img src="images/twitter.png" width="50" height="50" alt=""/></a> <a href="https://www.linkedin.com/in/darren-cosker-61271413?trk=hp-identity-name"><img src="images/linkedin.png" width="50" height="50" alt=""/></a> 
                <div id="welcome" style="text-align: left"> 
                    <div class="title"> 
                        <!--<h2>Home</h2>-->                         
                    </div>                     
                    <p>I am a Principal Scientist Manager at Microsoft (since 2021) - based out of the Cambridge, UK office - where I work on AI for human understanding for Presence and Mixed Reality - in products such as Teams and Microsoft Mesh.</p> 
                    <p>Up to 2021 I was a full time Professor Computer Science at the University of Bath, where I joined as a Royal Academy of Engineering/ EPSRC Research Fellow in 2007 and still hold a small part-time position. I have been fortunate enough to be awarded two previous Research Fellowships: Royal Academy of Engineering, 2007-2012, Royal Society Industry Fellowship (with Double Negative Visual Effects), 2012-2016.</p> 
                    <p> In 2015 I founded and was Director of the <a>Centre for the Analysis of Motion, Entertainment Research and Applications (CAMERA)</a>, funded by EPSRC/AHRC, with partner contributions from The Imaginarium, The Foundry, British Skeleton, Ministry of Defence and British Maritime Technologies. In 2020 I led the successful re-funding of CAMERA for another 5 years (EPSRC) and helped secure the £45m+ MyWorld project in the South West of the UK. <p/> <p>My primary research interest has always been on human motion analysis, recognition and synthesis - particulary for animation with applications in Video Games, Virtual Worlds, Mixed Reality and Movies.</p> <!--<p><font color="#FF0004"><b>PhD/Post-Doctoral Researcher Opportunities!</b></font>I am currently looking for a post-doctoral researcher with a background in 3D computer vision, ideally with application areas/interests in real time tracking. There are also PhD opportunities within CAMERA in several areas across motion capture and animation. Experience/interest in Computer Vision, Graphics and/or machine learning is desirable. Please email me for enquiries.</p>-->  
                </div>                 
                <div class="title"> 
                    <h2>Ship it! Production and Commercial Projects</h2> <a name="commercial"></a> 
                </div>                 
                <div id="commercial"> 
                    <p>I have always been a believer that you shouldn't do research and then just 'throw it over the wall' for others to take to real users and customers. In fact, most of the hard - and really interesting work - starts when you get your great reseach ideas into real people hands. Then you find out a few things - does it really work on real data? Is it robust? You will discover dozens of things that are wrong,  could be improved, and even better a set of new problems. At Microsoft I have been fortunate to get research into the hands of millions of customers through Mesh and Teams. In <a href="http://www.camera.ac.uk">CAMERA</a> I created a model whereby we translated research into impact by creating tools from our research, deploying them into our studio, and then delivering projects to clients that leverage these tools. Below is a snapshot of some of the projects I have been fortunate enough to be involved in!</p> 
                    <table width="484" border="0" align="center"> 
                        <tbody> 
                            <tr> 
                                <td> 
                                    <img src="images/Mesh_Time.jpg" alt="" width="244" height="262"/> 
                                </td>
                                <td> 
                                    <img src="images/Mesh_Avatars.jpg" alt="" width="244" height="252"/> 
                                </td>                                 
                                <td> 
                                    <img src="images/Mesh_Immersive.jpg" alt="" width="220" height="200"/> 
                                </td>                                 
                            </tr>                             
                            <tr> 
                                <td><b>Microsoft Mesh</b> - Mesh is a platform that allows people to connect in virtual worlds - on desktop, or VR.</td> 
                                <td><b>Mesh Avatars in Teams</b> - Mesh Avatars are animated by voice using AI technology developed by myself and my team at Microsoft. </td>
                                <td><b>Immersive Meetings</b> - The Avatar technology and AI audio features are also shipped in 3D experiences on Quest and Windows. </td>
                            </tr>                             
                            <tr> 
                                <td> 
                                    <img src="images/Aardman.jpg" alt="" width="260" height="120"/> 
                                </td>                                 
                                <td> 
                                    <img src="images/BBC.jpg" alt="" width="240" height="110"/> 
                                </td>                                 
                                <td> 
                                    <img src="images/Cosmos.jpg" alt="" width="240" height="130"/> 
                                </td>                                 
                            </tr>                             
                            <tr> 
                                <td><b>11:11 Memories Retold</b> - with Aardman and Bandai Namco (BAFTA nominated). We delivered all motion capture for the video game at our CAMERA studio.</td> 
                                <td><b>'Is Anna OK?'</b> - with BBC and Aardman. An immersive experience delivered with our in-house facial rigging, animation and motion capture solution.</td> 
                                <td><b>Cosmos Within Us</b> - with Satore Studios (Cannes Lion Winner). We build digital doubles for performers and animated them using our in house tools.</td> 
                            </tr>                             
                        </tbody>                         
                    </table>                     
                </div>                 <br> 
                <div class="title"> 
                    <h2>Research Themes</h2> <a name="areas"></a> 
                </div>                 
                <p>I have always been interested in people - and understanding how they move so we can model this and then learn to create better animation systems. This involves peoples faces, bodies, the environment, each other - and then the application to video games, movies, remote communication. I believe motion should always be higher fidelity than appearance - and poor appearance and great motion will always be better than great appearance and poor motion.</p> 
                <p>Along this direction I have explored many projects in the past - both in academia and industry - and established new teams as well as multiple commercial grade motion capture production studios. Below is an overview of my research based on different areas I have worked on. It is a representative list of papers only by broad topic area - for a full list see the <a href="#papers">research papers</a> section.</p> 
                <table width="484" border="0" align="center"> 
                    <tbody> 
                        <h3>Facial Analysis and Synthesis</h3> 
                        <tr> 
                            <td width="88"> 
                                <img src="images/CGF_2019.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                            <td> 
                                <img src="images/rigidity.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                            <td width="89"> 
                                <img src="images/MIG_2016.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                            <td width="89"> 
                                <img src="images/GI_2016.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                            <td width="90"> 
                                <img src="images/iccv.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                            <td width="90"> 
                                <img src="images/Vedran_IEEE.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                            <td> 
                                <img src="images/Trellis.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                        </tr>                         
                        <tr> 
                            <td>Evolutionary Facial Animation - CGF 2019</td> 
                            <td>Content Aware Deformation - CVMP 2015</td> 
                            <td>Procedural Facial Animation - CGF 2018</td> 
                            <td>Reading Between the Dots: Facial Capture- GI 2017</td> 
                            <td>Dynamic Morphable Models: D3DFACS - ICCV 2011</td> 
                            <td>4D Facial Movement for Biometrics - IEEE SMC 2010</td> 
                            <td>Speech Driven Facial Animation - ICPR 2004</td> 
                        </tr>                         
                    </tbody>                     
                </table>                 
                <table width="484" border="0" align="center"> 
                    <tbody> 
                        <h3>Performance Capture and Animation</h3> 
                        <tr> 
                            <td> 
                                <img src="images/RGBDDog.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                            <td> 
                                <img src="images/CGI_2019.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                            <td> 
                                <img src="images/SPORT_2018.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                            <td> 
                                <img src="images/FOOT_2018.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                            <td> 
                                <img src="images/MIG_2018.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                        </tr>                         
                        <tr> 
                            <td>RGBD-Dog: Predicting Canine Pose from RGBD  - CVPR 2020</td> 
                            <td>Scale Aware Performance Retargeting - CGI 2019</td> 
                            <td>Markerless Motion Capture Survey - Sports Medicine 2018</td> 
                            <td>Markerless Sprint Analysis - WACV 2018</td> 
                            <td>Elastic Deformation - MIG 2018</td> 
                        </tr>                         
                    </tbody>                     
                </table>                 
                <table width="484" border="0" align="center"> 
                    <tbody> 
                        <h3>Image and Video Processing</h3> 
                        <tr> 
                            <td> 
                                <img src="images/NIPS_2018.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                            <td> 
                                <img src="images/CVPR_2018.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                            <td> 
                                <img src="images/blur_robust.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                            <td> 
                                <img src="images/inferring.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                            <td> 
                                <img src="images/bmvc14.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                            <td> 
                                <img src="images/anchor_patch.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                        </tr>                         
                        <tr> 
                            <td>Image to Image Translation - NeurIPS 2018</td> 
                            <td>Multi-task Learning - CVPR 2018</td> 
                            <td>Blur Robust Robust Optical Flow - PR 2017</td> 
                            <td>Inferring Focal Length - CG 2015</td> 
                            <td>Shadow Removal - BMVC 2014</td> 
                            <td>Robust Feature Tracking - ACCV 2013</td> 
                        </tr>                         
                        <tr> 
                            <td></td> 
                            <td> 
                                <img src="images/digipro.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                            <td> 
                                <img src="images/LME_Faces.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                            <td> 
                                <img src="images/RAL_2016.bmp" alt="" width="100" height="80"/> 
                            </td>                             
                            <td> 
                                <img src="images/TVCG_Water.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                        </tr>                         
                        <tr> 
                            <td></td> 
                            <td>Camera Tracking in Visual Effects - DigiPro 2016</td> 
                            <td>Mesh based Optical Flow - CVPR 2013</td> 
                            <td>Non-Rigid Optical Flow Ground Truth - RAL 2016</td> 
                            <td>Water Reconstruction from Video - TVCG 2015</td> 
                        </tr>                         
                    </tbody>                     
                </table>                 
                <table width="484" border="0" align="center"> 
                    <tbody> 
                        <h3>Applied Perception for Vision and Graphics</h3> 
                        <tr> 
                            <td> 
                                <img src="images/APGV_2010.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                            <td> 
                                <img src="images/smile_traj_.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                            <td> 
                                <img src="images/McGurk.png" alt="" width="100" height="80"/> 
                            </td>                             
                            <td> 
                                <img src="images/sap_2016.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                            <td> 
                                <img src="images/hci.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                        </tr>                         
                        <tr> 
                            <td>Nonlinear 4D Facial Perception - ACM APGV / SAP 2011</td> 
                            <td>Facial Dynamics and Trustworthiness - Emotion 2008 </td> 
                            <td>Perceptual Evaluation of Video Based Facial Animation - ACM TAP 2005</td> 
                            <td>Evaluation of Foveated Rendering Methods - ACM SAP 2016</td> 
                            <td>Evaluation of Gesture Based Interfaces - Pervasive 2011</td> 
                        </tr>                         
                    </tbody>                     
                </table>                 
                <table width="484" border="0" align="center"> 
                    <tbody> 
                        <h3>Virtual and Augmented Reality</h3> 
                        <tr> 
                            <td> 
                                <img src="images/IEEEVR.jpg" alt="" width="75" height="75"/> 
                            </td>                             
                            <td> 
                                <img src="images/ISMAR_2019.jpg" alt="" width="75" height="75"/> 
                            </td>                             
                            <td> 
                                <img src="images/SIGGRAPH_2019.jpg" alt="" width="75" height="75"/> 
                            </td>                             
                            <td> 
                                <img src="images/SCA_2019.jpg" alt="" width="75" height="75"/> 
                            </td>                             
                            <td> 
                                <img src="images/vrst.jpg" alt="" width="75" height="75"/> 
                            </td>                             
                            <td> 
                                <img src="images/u4.jpeg" alt="" width="100" height="80"/> 
                            </td>                             
                        </tr>                         
                        <tr> 
                            <td>Real World Objects for Egocentric VR - IEEEVR 2020</td> 
                            <td>Creating Virtual Props - ISMAR 2019</td> 
                            <td>Real Time Object Deformation for VR - SIGGRAPH 2019 (poster)</td> 
                            <td>Multi-Camera / RGBD Object Tracking - SCA 2019</td> 
                            <td>Tracking Head Mounted Displays - VRST 2014</td> 
                            <td>Latency Aware Foveated Rendering - CVMP 2015</td> 
                        </tr>                         
                    </tbody>                     
                </table>                 <br> <br> 
                <div class="title"> 
                    <h2>Alumni</h2> <a name="team"></a> 
                </div>                 
                <div id="team"> 
                    <p>At Microsoft I lead a team of amazingly talented scientists and engineers. But as a Professor and previous Director of CAMERA I was also fortunate to work with some amazing students, researchers and engineers.</p> 
                    <p> Martin Parsons (CAMERA),  Murray Evans (CAMERA), Yiguo Qiao (Living With/RUH/InnovateUK), Jack Saunders, George Fletcher, Jake Deane, Kyle Reed (Cubic Motion); Jose Serra (Digital Domain/ ILM), Anamaria Ciucanu (MMU), Pedro Mendes, Shridhar Ravikumar (Amazon, Apple); Alastair Barber (The Foundry); Wenbin Li (Bath); Han Gong (Apple); Charalampos Koniaris (Disney Research); Daniel Beale; Sinan Mutlu (Framestore); Nicholas Swafford; Nadejda Roubtsova (CAMERA); Sinead Kearney (CAMERA); Maryam Naghizadeh; Catherine Taylor (Marshmallow Laser Feast)</p> 
                    <bl> </bl>                     
                    <ul class="actions"> 
                        <li> <a href="#" class="button">Etiam posuere</a> 
                        </li>                         
                    </ul>                     
                </div>                 
                <div class="title"> 
                    <h2>Research Funding and Awards</h2> <a name="funding"></a> 
                </div>                 
                <div id="grants"> 
                    <p>(PI) 2020-2025: CAMERA 2.0 - Centre for the Analysis of Motion, Entertainment Research and Applications (£4,151,614 FEC). <strong>EPSRC</strong></p> 
                    <p>(PI) 2019-2021: CAMERA Motion Capture Innovation Studio (£901,391) <strong>Horizon 2020</strong></p> 
                    <p>(PI) 2019-2022: A tool to reveal Individual Differences in Facial Perception (£402,113) <strong>Medical Research Council (MRC) </strong></p> 
                    <p>(PI) 2018-2020: Rheumatoid Arthritis Flare Profiler (£165,126, Total project value £663,290). Partners: Living With, NHS. <strong>InnovateUK</strong></p> 
                    <p>(Co-I) 2018-2022: Bristol and Bath Creative Cluster (~£4m). Partners: UWE, University of Bristol, Bath Spa University. <strong>AHRC</strong></p> 
                    <p>(PI) 2017-2019: DOVE: Deformable Objects for Virtual Environments (£128,746, Total project value £562,559 FEC). Partner: Marshmallow Laser Feast, Heston's Fat Duck. <strong>Innovate UK</strong></p> 
                    <p>(PI) 2016-2018: HARPC: HMC for Augmented Reality Performance Capture (£119,025, Total project value £517,616 FEC). Partner: The Imaginarium. <strong>Innovate UK</strong></p> 
                    <p><p>(PI) 2015-2020: Centre for the Analysis of Motion, Entertainment Research and Applications - CAMERA (£ 4,998,728 FEC). Partners: The Imaginarium, The Foundry, Ministry of Defence, British Maratime Technologies, British Skeleton. <strong>EPSRC/AHRC</strong>. (not including partner contributions, ~£5,000,000).</p> <p>(PI) 2015-2017: Biped to Animal (£108,109 FEC). Parter: The Imaginarium.<strong> Innovate UK.</strong></p> <p>(PI) 2015: Goal Oriented Real Time Intelligent Performance Retargeting &nbsp;(£29,997 FEC). Partner: The Imaginarium. <strong>Innovate UK</strong>. </p> <p>(Co-I) 2013-2016: Acquiring Complete and Editable Outdoor Models from Video and Images (£1,003,256 FEC).<strong> EPSRC</strong>.</p> <p>(PI-Bath) 2014-2017: Visual Image Interpretation in Man and Machine (VIIMM) (£121,030 FEC). Partner: University of Birmingham. <strong>EPSRC</strong></p> <p>(PI) 2012-2016: Next Generation Facial Capture and Animation (£100,887 FEC). Partner: Double Negative Visual Effects. The <strong>Royal Society Industry Fellowship</strong>. </p> <p>(PI) 2007-2012: Exploiting 4D Data for Creating Next Generation Facial Modelling and Animation Techniques (£460,640FEC). <strong>The Royal Academy of Engineering Research Fellowship</strong>. </p> <p>Other funding: PhD Studentships, EPSRC Innovation Acceleration Account (IAA), Nuffield Foundation. </p> <bl> </bl>  
                </div>                 
                <div class="title"> 
                    <h2>Code and Data</h2> <a name="datasets"></a> 
                </div>                 
                <div id="data" style="text-align: left"> 
                    <p><strong>RGBD-Dog</strong> RGBD-Dog contains motion capture and multiview (Sony) RGB and (Kinect) RGBD data for several dogs performing different actions (all cameras and mo-cap syncronised with calibration data included. You can get the data, code to view and the CVPR 2020 paper it is all based on from our <a href="https://github.com/CAMERA-Bath/RGBD-Dog">GitHub page</a>. In our CVPR 2020 paper we use the data to train a model to predict dog pose from RGBD data. However, it also works pretty well on other animals. In the future we will expand the data and code as we publish more of our research. </p> 
                    <p><b>D3DFACS</b> The D3DFACS Dataset contains over 500 FACS coded dynamic 3D (4D) sequences from 10 individuals - including 3D meshes, stereo UV maps, colour camera images and calibration files. You can find out more about it in our ICCV 2011 paper "<a href="ICCV_final_2011.pdf">A FACS Valid 3D Dynamic Action Unit Database with Applications to 3D Dynamic Morphable Facial Modelling</a>". If you would like to download the dataset for academic research, <a href="https://vision.cs.bath.ac.uk/facedata">please visit the data set website</a></p> 
                    <p><b>Shadow
			Removal Ground Truth and Evaluation</b> To encourage the open comparison of single image
                  shadow removal in community, we provide an online
                  benchmark site and a dataset. Our quantitatively
                  verified high quality dataset contains a wide range of
                  ground truth data (214 test cases in total). Each case
                  is rated according to 4 attributes, which are texture,
                  brokenness, colourfulness and softness, in 3
                  perceptual degrees from weak to strong. To access the
                  evaluation website, <a href="http://www.cs.bath.ac.uk/%7Ehg299/shadow_eval/">please visit here</a>.</p> 
                    <p></p> <br> <br> 
                </div>                 <br> 
                <div id="papers"> 
                    <div class="title"> 
                        <h2>Publications (Recent and Selected)</h2> <a name="papers"></a> 
                    </div>                     
                    <p>There are many better systems these days at keeping track of personal papers - e.g. my <a href="https://scholar.google.co.uk/citations?user=n0rYM4kAAAAJ&hl=en">Google Scholar page</a> or <a href="https://researchportal.bath.ac.uk/en/persons/darren-cosker/publications/">University of Bath Pure page</a>. So, I apologies if this page is not maintained as well as I would like and papers are missing!</p> 
                    <ul class="style1"> 
                        <!--
			<li>
			    <p class="date"><img src="images/book.png" alt="" width="75" height="50" /></p>
			    <h3>Automatic Structual Scene Digitalization </h3>
			    <p>R. Tang, Y. Wang, D. Cosker and W. Li</p>
			    <p>PLoS ONE, 2017</p>
            </li>
-->                                                                           
                        <p></p> 
                    </li>
                    <h3></h3> 
                    <p></p> 
                    <p></p> 
                </li>                                  
                <h3><a href="papers/Neurocomputing_Blur_2016.pdf">Blur Robust Optical Flow using Motion Channel</a></h3> 
                <p><a href="papers/Neurocomputing_Blur_2016.pdf">Wenbin Li, Yang Chen, JeeHang Lee, Gang Ren, Darren Cosker</a></p> 
                <p><a href="papers/Neurocomputing_Blur_2016.pdf">Neurocomputing 2016</a></p> 
            </li>                          
            <h3><a href="papers/JIFS_Drift_2016.pdf">Drift Robust Non-rigid Optical Flow Enhancement
 for Long Sequences</a></h3> 
            <p><a href="papers/JIFS_Drift_2016.pdf">Wenbin Li, Darren Cosker, Matthew Brown</a></p> 
            <p><a href="papers/JIFS_Drift_2016.pdf">Journal of Intelligent and Fuzzy Systems, JIFS 2016</a></p> 
        </li>                  
        <h3><a href="papers/VFX_2015.pdf">Facial Capture and Animation in Visual Effects</a></h3> 
        <p><a href="papers/VFX_2015.pdf">Darren Cosker, Peter Eisert and Volker Helzle</a></p> 
        <p><a href="papers/VFX_2015.pdf">'Digital Representations of the Real World: How to Capture, Model, and Render Visual Reality', CRC Press. Marcus A. Magnor, Oliver Grau, Olga Sorkine-Hornung, Christian Theobalt (Eds.), 2015</a></p> 
    </li>          
    <h3><a href="papers/CAG_2015.pdf">Inferring Changes in Intrinsic Parameters from Motion Blur</a></h3> 
    <p><a href="papers/CAG_2015.pdf">A Barber, M Brown, P Hogbin, D Cosker </a></p> 
    <p><a href="papers/CAG_2015.pdf">Computers &amp; Graphics, 52, 155-170, 2015</a></p> 
</li>  
<h3><a href="papers/CVMP_Nick_2015.pdf">Latency aware foveated rendering in unreal engine 4</a> </h3> 
<p><a href="papers/CVMP_Nick_2015.pdf">NT Swafford, D Cosker, K Mitchell </a></p> 
<p><a href="papers/CVMP_Nick_2015.pdf">Proceedings of the 12th European Conference on Visual Media Production (CVMP), 2015</a></p>  
<h3><a href="papers/CVMP_Babis_2015.pdf">Real-time variable rigidity texture mapping</a></h3> 
<p><a href="papers/CVMP_Babis_2015.pdf">C Koniaris, K Mitchell, D Cosker </a></p> 
<p><a href="papers/CVMP_Babis_2015.pdf">Proceedings of the 12th European Conference on Visual Media Production (CVMP), 2015</a></p>  
<h3><a href="papers/FAA_2015.pdf">Perceived Emotionality of Linear and Non-Linear AUs Synthesised using a 3D Dynamic Morphable Facial Model</a></h3> 
<p><a href="papers/FAA_2015.pdf">D Cosker, E Krumhuber, A Hilton</a></p> 
<p><a href="papers/FAA_2015.pdf">Facial Analysis and Animation/Audio Visual Speech Processing (FAAVSP), 2015</a></p> 
<li>
    <p class="date"></p>
</p> 
<h3><a href="papers/CVMP_Alastair_2014.pdf">Estimating camera intrinsics from motion blur</a></h3> 
<p><a href="papers/CVMP_Alastair_2014.pdf">A Barber, M Brown, P Hogbin, D Cosker </a></p> 
<p><a href="papers/CVMP_Alastair_2014.pdf">Proceedings of the 11th European Conference on Visual Media Production (CVMP), 2014</a> </p>  
<h3><a href="papers/VRST_2014.pdf">Dual sensor filtering for robust tracking of head-mounted displays</a></h3> 
<p><a href="papers/VRST_2014.pdf">Nicholas T Swafford, Bastiaan J Boom, Kartic Subr, David Sinclair, Darren Cosker, Kenny Mitchell </a></p> 
<p><a href="papers/VRST_2014.pdf">ACM Symp. on VR Software and Technology (VRST), 2014</a></p>  
<h3><a href="papers/ACCV_2014.pdf">Interactive Shadow Editing from Single Images</a></h3> 
<p><a href="papers/ACCV_2014.pdf">H Gong, D Cosker </a></p> 
<p><a href="papers/ACCV_2014.pdf">Asian Conference on Computer Vision (ACCV), Workshops, 243-252, 2014</a></p>  
<h3><a href="papers/BMVC_2014.pdf">Interactive shadow removal and ground truth for variable scene categories (<strong>BEST STUDENT PAPER PRIZE</strong>)</a></h3> 
<p><a href="papers/BMVC_2014.pdf">H Gong, D Cosker </a></p> 
<p><a href="papers/BMVC_2014.pdf">British Machine Vision Conference (BMVC), 2014</a></p>  
<h3><a href="papers/JCGT_2014.pdf">Survey of Texture Mapping Techniques for Representing and Rendering Volumetric Mesostructure</a></h3> 
<p><a href="papers/JCGT_2014.pdf">C Koniaris, D Cosker, X Yang, K Mitchell </a></p> 
<p><a href="papers/JCGT_2014.pdf">Journal of Computer Graphics Techniques, 2014</a></p>  
<h3><a href="papers/WACV_2014.pdf">Robust optical flow estimation for continuous blurred scenes using rgb-motion imaging and directional filtering <strong>(BEST STUDENT PAPER PRIZE)</strong></a></h3> 
<p><a href="papers/WACV_2014.pdf">W Li, Y Chen, JH Lee, G Ren, D Cosker </a></p> 
<p><a href="papers/WACV_2014.pdf">IEEE Winter Conf. on Applications of Comp. Vis. (WACV),792-799, 2014</a></p> 
<li>
    <p class="date"></p>
</p> 
<h3><a href="papers/CVMP_2013_Babis.pdf">Real-time content-aware texturing for deformable surfaces</a></h3> 
<p><a href="papers/CVMP_2013_Babis.pdf">C Koniaris, D Cosker, X Yang, K Mitchell, I Matthews </a></p> 
<p><a href="papers/CVMP_2013_Babis.pdf">Proceedings of the 10th European Conference on Visual Media Production (CVMP), 2013</a></p>  
<h3><a href="papers/MM_2013.pdf">Applications of face analysis and modeling in media production</a></h3> 
<p><a href="papers/MM_2013.pdf">D Cosker, P Eisert, O Grau, PJB Hancock, J McKinnell, EJ Ong </a></p> 
<p><a href="papers/MM_2013.pdf">IEEE Multimedia, 20 (4), 18-27, 2013</a></p>  
<h3><a href="papers/ICME_2013.pdf">User-aided single image shadow removal</a></h3> 
<p><a href="papers/ICME_2013.pdf">H Gong, D Cosker, C Li, M Brown </a></p> 
<p><a href="papers/ICME_2013.pdf">IEEE International Conference on Multimedia and Expo (ICME), 2013 </a></p>  
<h3><a href="papers/TVCG_2013.pdf">Water surface modeling from a single viewpoint video</a></h3> 
<p><a href="papers/TVCG_2013.pdf">C Li, D Pickup, T Saunders, D Cosker, D Marshall, PS Hall, P Willis </a></p> 
<p><a href="papers/TVCG_2013.pdf">IEEE Transactions onVisualization and Computer Graphics, &nbsp;19 (7), 1242-1251, 2013</a></p>  
<h3><a href="papers/CVPR_2013.pdf">Optical flow estimation using laplacian mesh energy</a></h3> 
<p><a href="papers/CVPR_2013.pdf">W Li, D Cosker, M Brown, R Tang </a></p> 
<p><a href="papers/CVPR_2013.pdf">IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2013 </a></p>  
<h3>Content aware texture mapping on deformable surfaces</h3> 
<p><a href="#">C Koniaris, D Cosker, X Yang, KJ Mitchell, I Matthews </a></p> 
<p>US Patent App. 13/838,840 </p>  
<h3><a href="papers/ACCV_2013.pdf">An anchor patch based optimization framework for reducing optical flow drift in long image sequences</a></h3> 
<p><a href="papers/ACCV_2013.pdf">W Li, D Cosker, M Brown </a></p> 
<p><a href="papers/ACCV_2013.pdf">Asian Conference on Computer Vision (ACCV), 112-125, 2013</a></p>  
<h3><a href="papers/CVMP_2011.pdf">Realtime video based water surface approximation</a></h3> 
<p><a href="papers/CVMP_2011.pdf">C Li, M Shaw, D Pickup, D Cosker, P Willis, P Hall </a></p> 
<p><a href="papers/CVMP_2011.pdf">European Conference in Visual Media Production (CVMP), 109-117, 2011 </a><br/> </p>  
<h3><a href="papers/ICCV_2011.pdf">A FACS valid 3D dynamic action unit database with applications to 3D dynamic morphable facial modeling</a></h3> 
<p><a href="papers/ICCV_2011.pdf">D Cosker, E Krumhuber, A Hilton </a></p> 
<p><a href="papers/ICCV_2011.pdf">IEEE International Conference on Computer Vision (ICCV), 2296-2303, 2011. </a></p>  
<h3>Identifying and evaluating gestural interaction in ubiquitous and pervasive computing</h3> 
<p>Michael Wright, CJ Lin, Darren Cosker, Eamonn O'Neill, Paul Johnson </p> 
<p>Proceedings GW2011: The 9th International Gesture Workshop: Gesture in Embodied Communication and Human-Computer Interaction, 2011</p>  
<h3><a href="papers/ACCV_2011.pdf">Reconstructing mass-conserved water surfaces using shape from shading and optical flow</a></h3> 
<p><a href="papers/ACCV_2011.pdf">D Pickup, C Li, D Cosker, P Hall, P Willis </a></p> 
<p><a href="papers/ACCV_2011.pdf">Asian Conference on Computer Vision (ACCV), 189-201, 2011</a></p>  
<h3><a href="papers/Pervasive_2011.pdf">3D gesture recognition: an evaluation of user and system performance</a></h3> 
<p><a href="papers/Pervasive_2011.pdf">M Wright, CJ Lin, E O’Neill, D Cosker, P Johnson </a></p> 
<p><a href="papers/Pervasive_2011.pdf">Pervasive Computing. Lecture Notes in Computer Science (LNCS), 294-313, 2011</a></p>  
<h3><a href="papers/FAA_2010.pdf">A FACS validated 3D human facial model</a></h3> 
<p><a href="papers/FAA_2010.pdf">D Cosker, E Krumhuber, A Hilton </a></p> 
<p><a href="papers/FAA_2010.pdf">Proceedings of the SSPNET 2nd International Symposium on Facial Analysis and Animation (FAA), 2010</a></p>  
<h3><a href="papers/APGV_2010.pdf">Perception of linear and nonlinear motion properties using a FACS validated 3D facial model</a></h3> 
<p><a href="papers/APGV_2010.pdf">D Cosker, E Krumhuber, A Hilton </a></p> 
<p><a href="papers/APGV_2010.pdf">Proceedings of the 7th Symposium on Applied Perception in Graphics and Visualisation (APGV), 2010</a></p>  
<h3><a href="http://onlinelibrary.wiley.com/doi/10.1002/9781118786642.ch17/summary">Facial Actions for Biometric Applications</a></h3> 
<p><a href="http://onlinelibrary.wiley.com/doi/10.1002/9781118786642.ch17/summary">Lanthao Benedikt, Paul L. Rosin, David Marshall, Darren Cosker, Hashmat Popat, Stephen Richmond</a></p> 
<p><a href="http://onlinelibrary.wiley.com/doi/10.1002/9781118786642.ch17/summary">'3D Imaging for Orthodontics and Maxillofacial Surgery', Eds. Chung How Kau, Stephen Richmond, 267-285, Wiley-Blackwell, 2010</a></p>  
<h3><a href="papers/IEEE_SMA_2010.pdf">Assessing the uniqueness and permanence of facial actions for use in biometric applications</a></h3> 
<p><a href="papers/IEEE_SMA_2010.pdf">Lanthao Benedikt, Darren Cosker, Paul L Rosin, David Marshall </a></p> 
<p><a href="papers/IEEE_SMA_2010.pdf"> IEEE Transactions on Systems, Man and Cybernetics, Part A: Systems and Humans, Volume 40, Issue 3, 449-460, 2010</a></p>  
<h3><a href="papers/JNVB_2009.pdf">Effects of dynamic attributes of smiles in human and synthetic faces: A simulated job interview setting</a></h3> 
<p><a href="papers/JNVB_2009.pdf">E Krumhuber, ASR Manstead, D Cosker, D Marshall, PL Rosin </a></p> 
<p><a href="papers/JNVB_2009.pdf">Journal of Nonverbal Behavior 33 (1), 1-15, 2009</a></p>  
<h3><a href="papers/BMVC_2009.pdf">Incremental Learning of Dynamical Models of Faces</a></h3> 
<p><a href="papers/BMVC_2009.pdf">C. Charron, Y. A. Hicks, P. Hall and D. Cosker </a></p> 
<p><a href="papers/BMVC_2009.pdf">In Proc. British Machine Vision Conference (BMVC), 2009.</a></p>  
<h3><a href="papers/CASA_2009_Long.pdf">Laughing, crying, sneezing and yawning: Automatic voice driven animation of non-speech articulations</a></h3> 
<p><a href="papers/CASA_2009_Long.pdf">D Cosker, J Edge </a></p> 
<p><a href="papers/CASA_2009_Long.pdf">Proceedings of Computer Animation and Social Agents (CASA), 2009</a> </p>  
<h3><a href="papers/Ortho_2008.pdf">Three‐dimensional motion analysis–an exploratory study. Part 1: Assessment of facial movement</a></h3> 
<p><a href="papers/Ortho_2008.pdf">H Popat, S Richmond, R Playle, D Marshall, PL Rosin, D Cosker </a></p> 
<p><a href="papers/Ortho_2008.pdf">Orthodontics &amp; craniofacial research 11 (4), 216-223, 2008</a></p>  
<h3><a href="papers/BTAS_2008.pdf">3D Facial Gestures in Biometrics: from Feasibility Study to Application</a></h3> 
<p><a href="papers/BTAS_2008.pdf">L. Benedikt, D. Cosker, D. Marshall, P. L. Rosin</a></p> 
<p><a href="papers/BTAS_2008.pdf">In Proc. of IEEE International Conference on Biometrics Theory, Applications and Systems (BTAS), pp 1-6, 2008</a></p>  
<h3><a href="papers/BMVC_2008.pdf">Facial Dynamics in Biometric Identification</a></h3> 
<p><a href="papers/BMVC_2008.pdf">L Benedikt, V Kajic, D Cosker, PL Rosin, AD Marshall </a></p> 
<p><a href="papers/BMVC_2008.pdf">In Proc. of British Machine Vision Conference (BMVC), 1-10, 2008</a></p>  
<h3><a href="papers/LNCS_2007.pdf">Re-mapping animation parameters between multiple types of facial model</a></h3> 
<p><a href="papers/LNCS_2007.pdf">D Cosker, R Borkett, D Marshall, PL Rosin</a></p> 
<p><a href="papers/LNCS_2007.pdf">Lecture Notes in Computer Science (LNCS), 4418, 365-378, 2007</a></p>  
<h3><a href="papers/CVMP_2007.pdf">Construction and perceptual evaluation of a 3D head model</a></h3> 
<p><a href="papers/CVMP_2007.pdf">L Benedikt, E Krumhuber, A Calvert, D Cosker, PL Rosin, D Marshall European Conference on Visual Media Production (CVMP), 2007. </a></p>  
<h3><a href="papers/Emotion_2007.pdf">Facial dynamics as indicators of trustworthiness and cooperative behavior</a></h3> 
<p><a href="papers/Emotion_2007.pdf">E Krumhuber, ASR Manstead, D Cosker, D Marshall, PL Rosin, A Kappas </a></p> 
<p><a href="papers/Emotion_2007.pdf">Emotion 7 (4), 730, 2007</a></p>  
<h3><a href="papers/SIGGRAPH_2007.pdf">Using Dynamic 3D Facial Data to Create 3D Appearance Models of Facial Action Units</a></h3> 
<p><a href="papers/SIGGRAPH_2007.pdf">L. Benedikt, E. Krumhuber, A. Calvert, D. Cosker, P. Rosin, D. Marshall ACM SIGGRAPH (poster), San Diego, 2007</a></p>       
<li> 
    <p class="date"><img src="images/Trellis.jpg" alt="" width="75" height="50"/></p> 
    <h3><a href="papers/ICPR_2004.pdf">Speech driven facial animation using a hidden markov coarticulation model</a></h3> 
    <p><a href="papers/ICPR_2004.pdf">D Cosker, D Marshall, PL Rosin, Y Hicks Pattern Recognition </a></p> 
    <p><a href="papers/ICPR_2004.pdf">17th IEEE International Conference on Pattern Recognition (ICPR), 2004</a></p> 
</li> 
<div class="title"> 
    <h2>Archive</h2> <a name="archive"></a> 
</div> 
<p>Below is a collection of previous activities - including workshops (at CVPR, ACCV, etc) and EPSRC research networks I have co-founded - kept here for future reference (mine as much as anything!)</p> <br> 
<h3>EPSRC Network on Visual Image Interpretation in Humans and Machines (ViiHM)</h3> 
<p> 
Understanding the environment via the sense of vision represents a challenging problem in computer science. Yet biological vision, as evidenced in the human visual system, seems to process the visual environment effortlessly. This supports the notion that understanding biological vision will help to solve problems in machine vision. However, some of the biggest advances in our understanding of human vision have occurred as a direct result of modern computing techniques. We can only really say we understand a complex system fully when we can recreate or simulate it, test hypotheses on the simulation, and take the simulation to the limits of its validity. 
The aims of the EPSRC VIIHM Network are: 
1.       To foster communication and joint projects between relevant research groups including those working on biological vision (human and non-human animals) computer vision and machine vision.
2.       To establish a series of grand challenges focused around well specified tasks where cross-over studies have a strong potential to provide robust solutions.
3.       To foster joint cross-discipline grant applications.
4.       To explore mechanisms to improve the utility of joint publications for both partners.
5.       To equip individual PhD and post-doctoral scientists to be future leaders of cross-over research projects.
6.       To establish a lasting vehicle for supporting cross-over biological and machine vision projects.
7.       To increase public engagement with the concept of biologically inspired computer vision. 
You can join and register for our first workshop at the same time or just join the Network here: http://www.viihm.org.uk</p> <br> 
<h3>EPSRC Network on Vision and Language (V&L Net)</h3> 
<p> 
The EPSRC Network on Vision and Language (V&L Net) is a forum for researchers from the fields of Computer Vision and Language Processing to meet, exchange ideas, expertise and technology, and form new partner- ships. Our aim is to create a lasting interdisciplinary research community situated at the language-vision interface, jointly working towards solutions for some of today's most challenging computational challenges, including image and video search, description of visual content and text-to-image generation.  
As a research collaboration forum, V&L Net has a real-life and a virtual dimension. We hold annual V&L Net meetings which combine the characteristics of an academic conference, a networking event and an exhibition. At the same time, the V&L Net website offers a wide variety of different tools and resources including networking tools and repositories of publications, data resources and software tools <p> <br> <h3>2nd Meeting of the EPSRC Network on Visual Image Interpretation in Humans and Machines (ViiHM), July 1st/2nd, 2015</h3> <p>We invite all academics and relevant industrial practitioners interested in the fostering of human and computer vision research to the first annual meeting of the EPSRC VIIHM Network. The annual meeting will focus on community building and will comprise of plenary talks from internationally renowned human and computer vision researchers, networking and community building opportunities and poster sessions. 
Full workshop details may be found here http://www.viihm.org.uk/home/events/second-workshop/</p> <br> <h3>2nd Workshop on User Centric Computer Vision (UCCV), 2014</h3> <p> 
UCCV 2014 is a workshop dedicated to research on interactive computer vision and methods for making computer vision more accessible to wider audiences. The workshop welcomes work on case studies, end-user applications, developer-centred approaches and many other aspects of computer vision.  
1st Meeting of the EPSRC Network on Visual Image Interpretation in Humans and Machines (ViiHM), September 24th-25th, 2014 
We invite all academics and relevant industrial practitioners interested in the fostering of human and computer vision research to the first annual meeting of the EPSRC VIIHM Network. The annual meeting will focus on community building and will comprise of plenary talks from internationally renowned human and computer vision researchers, networking and community building opportunities and poster sessions. 
Up to 80 applicants will then be invited to attend the workshop - based on a balance of early, mid and advanced career researchers. We will also aim to balance the mix of disciplines.
The meeting duration is from midday on the 24th to the afternoon of 25th September. Those interested should complete the form below and send it to the Network Administrator by 30th June 2014. Workshop applicants may optionally request a poster presentation using the same form. Posters may represent new work, a review of past work, an outline of planned work or position piece, or an outline of collaboration interests and opportunities. 
Full workshop details may be found here http://viihm.org.uk/workshop.html</p> <br> <h3>3rd Workshop On Vision And Language 2014 (VL'14), Dublin, 23rd August 2014 </h3> <p> 
Fragments of natural language, in the form of tags, captions, subtitles, surrounding text or audio, can aid the interpretation of image and video data by adding context or disambiguating visual appearance. In addition, labelled images are essential for training object or activity classifiers. On the other hand, visual data can help resolve challenges in language processing such as word sense disambiguation. Studying language and vision together can also provide new insight into cognition and universal representations of knowledge and meaning. Meanwhile, sign language and gestures are languages that require visual interpretation.  
We welcome papers describing original research combining language and vision. To encourage the sharing of novel and emerging ideas we also welcome papers describing new datasets, grand challenges, open problems, benchmarks and work in progress as well as survey papers. 
Full workshop details may be found here https://vision.cs.bath.ac.uk/VL_2014/ </p> <br> <h3>EPSRC Workshop on Vision and Language (2010-2013)</h3> <p> 
The EPSRC Network on Vision and Language (V&L Net) is a forum for researchers from the fields of Computer Vision and Language Processing to meet, exchange ideas, expertise and technology, and form new partner- ships. Our aim is to create a lasting interdisciplinary research community situated at the language-vision interface, jointly working towards solutions for some of today's most challenging computational challenges, including image and video search, description of visual content and text-to-image generation.  
As a research collaboration forum, V&L Net has a real-life and a virtual dimension. We hold annual V&L Net meetings which combine the characteristics of an academic conference, a networking event and an exhibition. At the same time, the V&L Net website offers a wide variety of different tools and resources including networking tools and repositories of publications, data resources and software tools. The networks home page may be found here. </p> <br> <h3>Eurographics UK - Theory and Practice of Computer Graphics 2013</h3> <p> 
The 31st Conference organised by the UK chapter of the Eurographics Association took place at the University of Bath on the 5-6 September 2013. The aim of this conference is to focus on theoretical and practical aspects of Computer Graphics and to bring together top practitioners, users and researchers, which will inspire further collaboration between participants particularly between academia and industry.  
The meeting website contains more details of the event.</p> <h3>IEEE CVPR Workshop on Vision and Language 2013</h3> <p> 
The EPSRC Network on Vision and Language (V&L Net) has been set up to foster collaborative work in this area. It is a forum for researchers from the fields of Computer Vision and Natural Language Processing to meet, exchange ideas, expertise and technology, and form new partnerships. The aim is to create a lasting interdisciplinary research community situated at the language-vision interface, jointly working towards solutions for some of today's most challenging computational challenges, including image and video search, description of visual content and text-to-image generation. A workshop on this theme - held jointly with CVPR - took place in 2013. 
The meeting website may be found here.</p> <br> <h3>Symposium on Facial Analysis and Animation (FAA), in Co-op with ACM, (2009, 2010, 2012)</h3> <p> 
The aim of this meeting is to bring together researchers and practitioners from both academia and industry – particularly in VFX and games - interested in all aspects of facial animation and related analysis. The meeting has previously been held in Edinburgh (2009/2010) and Vienna (2012). Watch this space for future meetings! </p> <h3>AVA/BMVA Biological and Computer Vision (2011,2012)</h3> <p> 
The study of biological and machine vision share much common history (e.g. Marr), and each discipline has benefited enormously from findings and techniques from the other.  In the UK (in contrast to elsewhere) the discussion and collaboration between the sister disciplines seems to have declined. The aim of this meeting, organised jointly by the Applied Vision Association (AVA) (UK biological vision) and British Machine Vision Association (BMVA) (UK computer vision) is to reignite conversations between these two fields. 
The meeting was held at Cardiff University (2011) and at Microsoft Research, Cambridge (2012). Watch this space for 2013 meeting information</p> <div id="copyright"> <span>&copy; Darren Cosker. All rights reserved. </span> <span>Design by <a href="http://templated.co" rel="nofollow">TEMPLATED</a>.</span> 
        </div> </div> </div>
