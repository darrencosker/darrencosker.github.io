<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"> 
<!--
Design by TEMPLATED
http://templated.co
Released for free under the Creative Commons Attribution License

Name       : Skeleton 
Description: A two-column, fixed-width design with dark color scheme.
Version    : 1.0
Released   : 20130902

--> 
<html xmlns="http://www.w3.org/1999/xhtml"> 
    <head> 
        <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/> 
        <title>Darren Cosker's Home Page</title>         
        <meta name="keywords" content=""/> 
        <meta name="description" content=""/> 
        <link href="../css/theme.css" rel="stylesheet" type="text/css"> 
        <link href="http://fonts.googleapis.com/css?family=Source+Sans+Pro:200,300,400,600,700,900" rel="stylesheet"/> 
        <link href="default.css" rel="stylesheet" type="text/css" media="all"/> 
        <link href="fonts.css" rel="stylesheet" type="text/css" media="all"/> 
        <!--[if IE 6]><link href="default_ie6.css" rel="stylesheet" type="text/css" /><![endif]-->         
    </head>     
    <body> 
        <div id="page" class="container"> 
            <div id="header"> 
                <div id="logo"> 
                    <img src="images/darren_7.jpg" alt="" width="203" height="345"/> 
                    <h2><a href="#">Prof. Darren Cosker</a></h2> 
                    <p><font color="white"><b>Principal Scientist Manager</b> @ Microsoft</font></p> 
                    <p><font color="white"><b>Professor</b> @ University of Bath</font></p> <span></span> 
                    <p><font color="white"><strong>Royal Society Industry Fellow</strong></font></p> <span></span> 
					<p><font color="white"><a href="https://scholar.google.com.sg/citations?hl=en&user=n0rYM4kAAAAJ&view_op=list_works&sortby=pubdate"><strong>Google Scholar Page</strong></a></font></p> <span></span> 
                  <p><font color="white"><strong>Contact</strong>: <a href="https://www.linkedin.com/in/darren-cosker-61271413/">LinkedIn</a> or <a href="https://twitter.com/dopomoc">X-Twitter</a></font></p> 
					<a href="#personal"><b>Personal</b></a> 
                </div>     
				<!--
                <div id="menu"> 
                    <ul> 
                        <li> <a href="http://www.camera.ac.uk">CAMERA</a> 
                        </li>                         
                        <li> <a href="#areas">RESEARCH AREAS</a> 
                        </li>                         
                        <li> <a href="#funding">FUNDING</a> 
                        </li>                         
                        <li> <a href="#team">TEAM</a> 
                        </li>                         
                        <li> <a href="#papers">PAPERS</a> 
                        </li>                         
                        <li> <a href="#commercial">COMMERCIAL</a> 
                        </li>                         
                        <li> <a href="#datasets">CODE+DATA</a> 
                        </li>                         
                        <li> <a href="#archive">ARCHIVE</a> 
                        </li>                         
                    </ul>                     
                </div>     
				-->
    </div>             
            <div id="main" align="center"> 
              
                <!--
		<table width="200" border="0" align="center" cellpadding="10">
  <tbody>
    <tr>
      <td><img src="images/darren_6.jpg" alt="" width="150" height="150" /></td>
      <td>Prof. Darren Cosker
          <p>D.P.Cosker@bath.ac.uk</p>
          <p>Professor/Royal Society Industry Fellow</p></td>
    </tr>
  </tbody>
</table>
-->                 <a href="http://www.camera.ac.uk"><img src="images/CAMERA_C_logo.jpg" width="55" height="50" width="50" height="50" alt=""/></a> <a href="https://github.com/dopomoc"><img src="images/github_2.png" width="50" height="50"></a> <a href="https://twitter.com/dopomoc"><img src="images/twitter.png" width="50" height="50" alt=""/></a> <a href="https://www.linkedin.com/in/darren-cosker-61271413?trk=hp-identity-name"><img src="images/linkedin.png" width="50" height="50" alt=""/></a> 
                <div id="welcome" style="text-align: left"> 
                    <div class="title"> 
                        <!--<h2>Home</h2>-->                         
                    </div>                     
                  <p>I am a Principal Scientist Manager at <strong>Microsoft</strong> (since 2021) - based in Cambridge (UK) - where I lead a team working on real time perception and generation of human motion for <strong> remote communication in 2D and Mixed Reality</strong>. Our work at Microsoft has been shipped in several products including <strong>Teams</strong> and <strong>Microsoft Mesh</strong>.</p> 
                    <p>Up to 2021 I was a full time <strong>Professor</strong> Computer Science at the University of Bath, where I joined as a Royal Academy of Engineering/ EPSRC Research Fellow in 2007 and still hold a  part-time position. I have been fortunate enough to be awarded two previous Research Fellowships: <strong>Royal Academy of Engineering</strong>, 2007-2012, <strong>Royal Society</strong> Industry Fellowship (with Double Negative Visual Effects), 2012-2016.</p> 
                    <p> In 2015 I <strong>founded</strong> and was <strong>Director of the <a>Centre for the Analysis of Motion, Entertainment Research and Applications (CAMERA</a></strong>, growing the centre from 0 to over 50 people, and raising over £20m in funding from the UK Research Council (EPSRC, AHRC), including contributions from partners including The Imaginarium, The Foundry, British Skeleton, Ministry of Defence and British Maritime Technologies. In 2020 - before stepping down as Director to join Microsoft - I led the successful re-funding of CAMERA for another 5 years (EPSRC) and helped secure the £45m+ MyWorld project in the South West of the UK.
                    <p/> <p>I'm interested in several different areas across research and product engineering (see sections below on Product and Research Impact). However, broadly these interests involve applying<strong> computer vision, graphics, generative AI and animation</strong> science and engineering to problems in <strong>Mixed Reality</strong> (Avatars, Presence and Communication), <strong>Motion Capture, Video Game Animation, Visual Effects</strong> amd <strong>Biomechanics</strong>.
                      <!--<p><font color="#FF0004"><b>PhD/Post-Doctoral Researcher Opportunities!</b></font>I am currently looking for a post-doctoral researcher with a background in 3D computer vision, ideally with application areas/interests in real time tracking. There are also PhD opportunities within CAMERA in several areas across motion capture and animation. Experience/interest in Computer Vision, Graphics and/or machine learning is desirable. Please email me for enquiries.</p>-->
                </div>                 
                <div class="title"> 
                    <h2>Product Impact: From Immersive Experiences to Video Games</h2> 
                    <a name="commercial"></a> 
                </div>                 
                <div id="commercial" style="text-align: left"> 
					<p>At <b>Microsoft</b> I have been fortunate to <strong>ship my work</strong> into the hands of millions of customers through <strong>Microsoft Mesh and Teams</strong>, where myself and my team have created and shipped real-time avatar animation technology from voice and video for remote and immersive meetings. <b>Microsoft Mesh</b> - Mesh is a platform that allows people to connect in virtual worlds - on desktop, or VR. A core part of Mesh are their Avatars - which you can use in immersive 3D meetings, or in standard 2D Teams calls.</p>
					<figure>
					<a href="https://www.microsoft.com/en-us/microsoft-teams/microsoft-mesh?msockid=159b57740a38607822b447e10b1f61b1"><img src="images/microsoft_1.jpg" alt="" width="860" height="300" align="centre"/></a>
						<figcaption>Microsoft Mesh allows you to experience meetings in 2D via Teams or VR using Meta Quest headsets.</figcaption>
				  </figure>
						
               
					<p>At <a href="http://www.camera.ac.uk"><b>CAMERA</b></a> I created multi-disciplary research environment with three themes - each converging on technology around human perception, analysis and synthesis: (1) Entertainment, (2) Human Performance Enhancement, and (3) Health & Rehabilitation. Each theme was partnered with industrial partners, giving us real applications and routes to impact. 
					</p>

					<figure>
					<a href="https://www.microsoft.com/en-us/microsoft-teams/microsoft-mesh?msockid=159b57740a38607822b447e10b1f61b1"><img src="images/camera_1.jpg" alt="" width="860" height="300" align="centre" /></a>
						<figcaption>CAMERA has three themes of research and multiple industrial partners (<strong>left</strong> - the original partners in 2015 and theme areas). As Director of CAMERA, I also established 2 industry standard motion capture and photogrammetry studios for creating and animating digital humans (<strong>right</strong>, the original CAMERA studio at the University of Bath).</figcaption>
					</figure>
				  <p>At CAMERA I also created a framework where we were able to get a large amount of real world impact from our research. We did they by creating production tools from our research with a team of engineers, deploying them into our studio, and then delivering projects - from motion capture to digital humans - to clients that leverage these tools. This resulted in helping ship a number of <strong>video games and award winning immersive experiences</strong>. Below is a snapshot of some of the shipped products and expreiences I have been fortunate enough to be involved in as lead or part of a wider team.</p> 
					<table width="484" border="0" align="center"> 
                        <tbody> 
                            <tr>
                            <tr> 
                                <td> 
                                    <img src="images/Aardman.jpg" alt="" width="260" height="120"/> 
                                </td>                                 
                                <td> 
                                    <img src="images/BBC.jpg" alt="" width="240" height="110"/> 
                                </td>                                 
                                <td> 
                                    <img src="images/Cosmos.jpg" alt="" width="240" height="150"/> 
                                </td>                                 
                            </tr>                             
                            <tr> 
                                <td><b>11:11 Memories Retold</b> - with Aardman and Bandai Namco (<strong>BAFTA nominated</strong>). We delivered all motion capture for the video game at our CAMERA studio (available on Playstation, XBox, PC)&nbsp; .</td> 
                                <td><b>'Is Anna OK?'</b> - with BBC and Aardman. An immersive experience delivered with our in-house facial rigging, animation and motion capture solution.</td> 
                                <td><b>Cosmos Within Us</b> - with Satore Studios (<strong>Cannes Lion Winner</strong>). We build digital doubles for performers and animated them using our in house tools.</td> 
                            </tr>                             
                        </tbody>                         
                    </table>                     
                </div>                 <br> 
                <div class="title"> 
                    <h2>Research Impact: From Digital Humans to Biomechanics and Human Perception</h2> <a name="areas"></a> 
                               
                <p align="left">My research has spanned several areas over the years - building digital humans, motion capture of faces, bodies and animals, animation, computer vision, biomechanics.. One of the central themes though has been motion - understanding this, measuring it, and finding ways to use this for different scenarios and applications. A list of my papers can be found on my <a href="https://scholar.google.com.sg/citations?hl=en&user=n0rYM4kAAAAJ&view_op=list_works&sortby=pubdate">Google Scholar page</a> (I don't maintain an up to date one here anymore). However, my research broadly has fallen into the following themes. </p>
                </div>  
				<figure>
					<img src="images/performance_cap_1.jpg" alt="" width="787" height="286" align="centre" />
						<figcaption align="left"><strong>Facial Performance Capture and Animation</strong>. The image shows several results spanning 4D facial capture (back in 2011), monocular facial capture, and performance retargeting from motion capture (in collaboration with The Imaginarium Studios). For the latter project, we built end to end pipelines to build rigged digital facial models automatically from single 3D scans, and then animate these from marker based capture systems (e.g. Vicon Cara, or optical markers) or single cameras. For 4D capture, we scanned a large comprehensive set of FACS sequences - the D4DFACS dataset used as one of the basis of the FLAME model from MaxPlanck. We also introduced a method to fit a single topology to the sequences using optical flow in UV space. </figcaption>
					</figure>
				
				<figure>
					<img src="images/generative_1.jpg" alt="" width="755" height="572" align="centre" />
						<figcaption align="left"><strong>Generative Models of Shape and Motion</strong>. My work has touched on several areas of Generative AI. My PhD (back in 2006) was on the topic of audio driven facial animation - one which is now receiving a great deal of attention. More recently we re-explored this area in a recent personalised animation paper (ICCV 2023 - see Google Scholar). I also ship audio driven avatar technology at Microsoft for Mesh (although using different approaches from the paper, which is not real time..). I have also worked on probabilistic models of shape - for example for human faces, bodies and animals (the image shows shape generation using Gaussian Process Latent Variable Models (GPLVMs) - though if you like deep learning instead you can also use a VAE or GAN if you like :-) </figcaption>
					</figure>
				
					<figure>
					<img src="images/synthetic_1.jpg" alt="" width="826" height="185" align="centre" />
						<figcaption align="left"><strong>Synthetic Data for AI and Computer Vision</strong>. We can use computer graphics to render realistic images of 3D models (faces, bodies, animals) with perfect labels for e.g., landmarks, depth maps and segmentation masks. We can then use this data to train powerful computer vision and AI models to perform a range of tasks. Synthetic data such as this has several advantages (1) we can generate millions of training pairs and avoid the need for any real data which can be timeconsuming to collect, (2) our labels are perfect and not prone to human error, and (3) we avoid ethical issues with collecting data from real people. The images show results of applying this to pose estimation and other computer vision tasks on animals. Although the same principal can be applied to humans (our team in Microsoft Cambridge (UK) specialises in this area).
					</figure>
						
					<figure>
					<img src="images/digital_humans_avatars_1.jpg" alt="" width="833" height="438" align="centre" />
						<figcaption align="left"><strong>Digital Humans and Avatars: Animation, Embodiment and Perception</strong>. In CAMERA we created pipelines to create digital humans that can be used for video games, animation research and VR research. For example, in the images above we can see results of a pipeline that begins with photogrammetry, fits a human rig (parametric human model) to the data, attaches texture (albedo colour), and then is compatible with any standard animation system in e.g., Unity or Unreal. We used this pipeline in several VR studies around embodiment, but also with CAMERA customers to create immersive experiences with e.g., Satore Studies and the BBC.
					</figure>
						
					<figure>
					<img src="images/biomechanics_1.jpg" alt="" width="833" height="458" align="centre" />
						<figcaption align="left"><strong>Markerless Motion Capture and Analysis for Biomechanics</strong>. Motion capture originated in biomechanics, and one of the core motivations of CAMERA was to take state of the art computer vision and AI techniques, and attempt to solve problems in this space. We worked closely with elite athletes and coaches (e.g., the British Skeleton Bobsleigh team) and developed systems to provide coaches with biomechanical data to measure performance without the need for time-consuming marker based systems which are common in the field. One outcome was a mobile 'tunnel' or cameras we could move into training scenarios to capture real athlete data, and record e.g., step frequency, 3D human pose, and other useful signals for coaches and athletes with the click of a button.
					</figure>
					<figure>
					<img src="images/human_motion_ego_1.jpg" alt="" width="833" height="458" align="centre" />
						<figcaption align="left"><strong>Human Pose Estimation from Egocentric CAMERAS and HMDs</strong>. At Microsoft, we have published work from our team and with collaborators that explore 3D human pose estimation while wearing HMDs (e.g. Meta Quest and HoloLens 2) or from egocentric cameras where a person might be ocluded. In both cases we have achieved state of the art results, and in the case of full body pose from HMDs achieved over 200 fps on CPUs - suitable for mobile devices such as HoloLens 2. These technologies are crucial for creating compelling presence experiences in Mixed Reality where the user embodies an avatar of themself.
					</figure>
                <!--
                <table width="484" border="0" align="center"> 
                    <tbody> 
                        <tr> 
                            <td> 
                                <img src="images/Mesh_Time.jpg" alt="" width="244" height="262"/> 
                            </td>                             
                            <td> 
                                <img src="images/Mesh_Avatars.jpg" alt="" width="244" height="210"/> 
                            </td>                             
                            <td> 
                                <img src="images/Mesh_Immersive.jpg" alt="" width="220" height="200"/> 
                            </td>                             
                        </tr>                         
                    </tbody>                     
                </table>         
                --!>        
                <!--
                <table width="484" border="0" align="center"> 
                    <tbody> 
                        <h3>Facial Analysis and Synthesis</h3> 
                        <tr> 
                            <td width="88"> 
                                <img src="images/CGF_2019.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                            <td> 
                                <img src="images/rigidity.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                            <td width="89"> 
                                <img src="images/MIG_2016.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                            <td width="89"> 
                                <img src="images/GI_2016.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                            <td width="90"> 
                                <img src="images/iccv.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                            <td width="90"> 
                                <img src="images/Vedran_IEEE.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                            <td> 
                                <img src="images/Trellis.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                        </tr>                         
                        <tr> 
                            <td>Evolutionary Facial Animation - CGF 2019</td> 
                            <td>Content Aware Deformation - CVMP 2015</td> 
                            <td>Procedural Facial Animation - CGF 2018</td> 
                            <td>Reading Between the Dots: Facial Capture- GI 2017</td> 
                            <td>Dynamic Morphable Models: D3DFACS - ICCV 2011</td> 
                            <td>4D Facial Movement for Biometrics - IEEE SMC 2010</td> 
                            <td>Speech Driven Facial Animation - ICPR 2004</td> 
                        </tr>                         
                    </tbody>                     
                </table>                 
                <table width="484" border="0" align="center"> 
                    <tbody> 
                        <h3>Performance Capture and Animation</h3> 
                        <tr> 
                            <td> 
                                <img src="images/RGBDDog.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                            <td> 
                                <img src="images/CGI_2019.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                            <td> 
                                <img src="images/SPORT_2018.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                            <td> 
                                <img src="images/FOOT_2018.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                            <td> 
                                <img src="images/MIG_2018.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                        </tr>                         
                        <tr> 
                            <td>RGBD-Dog: Predicting Canine Pose from RGBD  - CVPR 2020</td> 
                            <td>Scale Aware Performance Retargeting - CGI 2019</td> 
                            <td>Markerless Motion Capture Survey - Sports Medicine 2018</td> 
                            <td>Markerless Sprint Analysis - WACV 2018</td> 
                            <td>Elastic Deformation - MIG 2018</td> 
                        </tr>                         
                    </tbody>                     
                </table>                 
                <table width="484" border="0" align="center"> 
                    <tbody> 
                        <h3>Image and Video Processing</h3> 
                        <tr> 
                            <td> 
                                <img src="images/NIPS_2018.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                            <td> 
                                <img src="images/CVPR_2018.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                            <td> 
                                <img src="images/blur_robust.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                            <td> 
                                <img src="images/inferring.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                            <td> 
                                <img src="images/bmvc14.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                            <td> 
                                <img src="images/anchor_patch.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                        </tr>                         
                        <tr> 
                            <td>Image to Image Translation - NeurIPS 2018</td> 
                            <td>Multi-task Learning - CVPR 2018</td> 
                            <td>Blur Robust Robust Optical Flow - PR 2017</td> 
                            <td>Inferring Focal Length - CG 2015</td> 
                            <td>Shadow Removal - BMVC 2014</td> 
                            <td>Robust Feature Tracking - ACCV 2013</td> 
                        </tr>                         
                        <tr> 
                            <td></td> 
                            <td> 
                                <img src="images/digipro.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                            <td> 
                                <img src="images/LME_Faces.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                            <td> 
                                <img src="images/RAL_2016.bmp" alt="" width="100" height="80"/> 
                            </td>                             
                            <td> 
                                <img src="images/TVCG_Water.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                        </tr>                         
                        <tr> 
                            <td></td> 
                            <td>Camera Tracking in Visual Effects - DigiPro 2016</td> 
                            <td>Mesh based Optical Flow - CVPR 2013</td> 
                            <td>Non-Rigid Optical Flow Ground Truth - RAL 2016</td> 
                            <td>Water Reconstruction from Video - TVCG 2015</td> 
                        </tr>                         
                    </tbody>                     
                </table>                 
                <table width="484" border="0" align="center"> 
                    <tbody> 
                        <h3>Applied Perception for Vision and Graphics</h3> 
                        <tr> 
                            <td> 
                                <img src="images/APGV_2010.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                            <td> 
                                <img src="images/smile_traj_.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                            <td> 
                                <img src="images/McGurk.png" alt="" width="100" height="80"/> 
                            </td>                             
                            <td> 
                                <img src="images/sap_2016.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                            <td> 
                                <img src="images/hci.jpg" alt="" width="100" height="80"/> 
                            </td>                             
                        </tr>                         
                        <tr> 
                            <td>Nonlinear 4D Facial Perception - ACM APGV / SAP 2011</td> 
                            <td>Facial Dynamics and Trustworthiness - Emotion 2008 </td> 
                            <td>Perceptual Evaluation of Video Based Facial Animation - ACM TAP 2005</td> 
                            <td>Evaluation of Foveated Rendering Methods - ACM SAP 2016</td> 
                            <td>Evaluation of Gesture Based Interfaces - Pervasive 2011</td> 
                        </tr>                         
                    </tbody>                     
                </table>                 
                <table width="484" border="0" align="center"> 
                    <tbody> 
                        <h3>Virtual and Augmented Reality</h3> 
                        <tr> 
                            <td> 
                                <img src="images/IEEEVR.jpg" alt="" width="75" height="75"/> 
                            </td>                             
                            <td> 
                                <img src="images/ISMAR_2019.jpg" alt="" width="75" height="75"/> 
                            </td>                             
                            <td> 
                                <img src="images/SIGGRAPH_2019.jpg" alt="" width="75" height="75"/> 
                            </td>                             
                            <td> 
                                <img src="images/SCA_2019.jpg" alt="" width="75" height="75"/> 
                            </td>                             
                            <td> 
                                <img src="images/vrst.jpg" alt="" width="75" height="75"/> 
                            </td>                             
                            <td> 
                                <img src="images/u4.jpeg" alt="" width="100" height="80"/> 
                            </td>                             
                        </tr>                         
                        <tr> 
                            <td>Real World Objects for Egocentric VR - IEEEVR 2020</td> 
                            <td>Creating Virtual Props - ISMAR 2019</td> 
                            <td>Real Time Object Deformation for VR - SIGGRAPH 2019 (poster)</td> 
                            <td>Multi-Camera / RGBD Object Tracking - SCA 2019</td> 
                            <td>Tracking Head Mounted Displays - VRST 2014</td> 
                            <td>Latency Aware Foveated Rendering - CVMP 2015</td> 
                        </tr>                         
                    </tbody>                     
                </table>
                -->                 <br> <br> 
                               
                <div class="title"> 
                    <h2>Research Funding and Awards</h2> <a name="funding"></a> 
                </div>
				<p align="left">As a Professor one of the core activities in funding research activites. The primary way to do this is through government funded research councils. In the UK, the major body is UKRI, under which vertical activies are funded - e.g. EPSRC (Engineering, Physical Sciences - including Computer Science), and AHRC (Arts and Humanities). Below is a list of the major funding I recieved for my academic research - used to fund large scale research centres (e.g. CAMERA) and more targeted short-term activities.</p>
                <div id="grants" align="left"> 
					<p>(<strong>PI</strong> Uni. of Bath) 2021-2026: MyWorld (~£45 FEC). <strong>UKRI</strong></p> 
                    <p>(<strong>PI</strong>) 2020-2025: CAMERA 2.0 - Centre for the Analysis of Motion, Entertainment Research and Applications (£4,151,614 FEC). <strong>EPSRC</strong></p> 
                    <p>(<strong>PI</strong>) 2019-2021: CAMERA Motion Capture Innovation Studio (£901,391) <strong>Horizon 2020</strong></p> 
                    <p>(<strong>PI</strong>) 2019-2022: A tool to reveal Individual Differences in Facial Perception (£402,113) <strong>Medical Research Council (MRC) </strong></p> 
                    <p>(<strong>PI</strong>) 2018-2020: Rheumatoid Arthritis Flare Profiler (£165,126, Total project value £663,290). Partners: Living With, NHS. <strong>InnovateUK</strong></p> 
                    <p>(<strong>Co</strong>-I) 2018-2022: Bristol and Bath Creative Cluster (~£4m). Partners: UWE, University of Bristol, Bath Spa University. <strong>AHRC</strong></p> 
                    <p>(<strong>PI</strong>) 2017-2019: DOVE: Deformable Objects for Virtual Environments (£128,746, Total project value £562,559 FEC). Partner: Marshmallow Laser Feast, Heston's Fat Duck. <strong>Innovate UK</strong></p> 
                    <p>(<strong>PI</strong>) 2016-2018: HARPC: HMC for Augmented Reality Performance Capture (£119,025, Total project value £517,616 FEC). Partner: The Imaginarium. <strong>Innovate UK</strong></p> 
                    <p><p>(<strong>PI</strong>) 2015-2020: Centre for the Analysis of Motion, Entertainment Research and Applications - CAMERA (£ 4,998,728 FEC). Partners: The Imaginarium, The Foundry, Ministry of Defence, British Maratime Technologies, British Skeleton. <strong>EPSRC/AHRC</strong>. (not including partner contributions, ~£5,000,000).</p> 
                    <p>(<strong>PI</strong>) 2015-2017: Biped to Animal (£108,109 FEC). Parter: The Imaginarium.<strong> Innovate UK.</strong></p> 
                    <p>(<strong>PI</strong>) 2015: Goal Oriented Real Time Intelligent Performance Retargeting &nbsp;(£29,997 FEC). Partner: The Imaginarium. <strong>Innovate UK</strong>. </p> 
                    <p>(<strong>Co</strong>-I) 2013-2016: Acquiring Complete and Editable Outdoor Models from Video and Images (£1,003,256 FEC).<strong> EPSRC</strong>.</p> 
                    <p>(<strong>PI</strong>-Bath) 2014-2017: Visual Image Interpretation in Man and Machine (VIIMM) (£121,030 FEC). Partner: University of Birmingham. <strong>EPSRC</strong></p> 
                    <p>(<strong>PI</strong>) 2012-2016: Next Generation Facial Capture and Animation (£100,887 FEC). Partner: Double Negative Visual Effects. The <strong>Royal Society Industry Fellowship</strong>. </p> 
                    <p>(<strong>PI</strong>) 2007-2012: Exploiting 4D Data for Creating Next Generation Facial Modelling and Animation Techniques (£460,640FEC). <strong>The Royal Academy of Engineering Research Fellowship</strong>. </p> 
                    <p>Other funding: PhD Studentships, EPSRC Innovation Acceleration Account (IAA), Nuffield Foundation. </p> <bl> </bl>  
                </div>                 
                <div class="title"> 
                    <h2>Public Data</h2> <a name="datasets"></a> 
                </div>                 
                <div id="data" style="text-align: left"> 
					<p>Over the years I have collected and made public several datasets. Below are the major ones still in use today.</p>
                  <p><strong>RGBD-Dog</strong> RGBD-Dog contains motion capture and multiview (Sony) RGB and (Kinect) RGBD data for several dogs performing different actions (all cameras and mo-cap syncronised with calibration data included. You can get the data, code to view and the CVPR 2020 paper it is all based on from our <a href="https://github.com/CAMERA-Bath/RGBD-Dog">GitHub page</a>. In our CVPR 2020 paper we use the data to train a model to predict dog pose from RGBD data. However, it also works pretty well on other animals. In the future we will expand the data and code as we publish more of our research. </p> 
                    <p><b>D3DFACS</b> The D3DFACS Dataset contains over 500 FACS coded dynamic 3D (4D) sequences from 10 individuals - including 3D meshes, stereo UV maps, colour camera images and calibration files. You can find out more about it in our ICCV 2011 paper "<a href="ICCV_final_2011.pdf">A FACS Valid 3D Dynamic Action Unit Database with Applications to 3D Dynamic Morphable Facial Modelling</a>". If you would like to download the dataset for academic research, <a href="https://vision.cs.bath.ac.uk/facedata">please visit the data set website</a></p> 
                    <p><b>Shadow
			Removal Ground Truth and Evaluation</b> To encourage the open comparison of single image
                  shadow removal in community, we provide an online
                  benchmark site and a dataset. Our quantitatively
                  verified high quality dataset contains a wide range of
                  ground truth data (214 test cases in total). Each case
                  is rated according to 4 attributes, which are texture,
                  brokenness, colourfulness and softness, in 3
                  perceptual degrees from weak to strong. To access the
                  evaluation website, <a href="http://www.cs.bath.ac.uk/%7Ehg299/shadow_eval/">please visit here</a>.</p> 
                    <p></p> 
                </div>                 <br> 

					<div class="title"> 
                    <h2>Alumni (University)</h2> 
                    <a name="team"></a> 
                </div>                 
                <div id="team" align="left"> 
                    <p>At Microsoft I lead a team of amazingly talented scientists and engineers. But as a Professor and previous Director of CAMERA I have also fortunate to work with some amazing students, researchers and engineers.</p> 
                    <p> Martin Parsons (CAMERA),  Murray Evans (CAMERA), Yiguo Qiao (Living With/RUH/InnovateUK), Jack Saunders, George Fletcher, Jake Deane, Kyle Reed (Cubic Motion); Jose Serra (Digital Domain/ ILM), Anamaria Ciucanu (MMU), Pedro Mendes, Shridhar Ravikumar (Amazon, Apple); Alastair Barber (The Foundry); Wenbin Li (Bath); Han Gong (Apple); Charalampos Koniaris (Disney Research); Daniel Beale; Sinan Mutlu (Framestore); Nicholas Swafford; Nadejda Roubtsova (CAMERA); Sinead Kearney (CAMERA); Maryam Naghizadeh; Catherine Taylor (Marshmallow Laser Feast)</p> 
                                        
                </div>  
				<div class="title"> 
                    <h2>Personal</h2> 
                    <a name="personal"></a> 
                </div> 
				<div id="personal" align="left"> 
				
					<p>I love my work, but of course the number one thing in my life is my family. If anything, having a family motives me even more in my work - giving me the desire to make sure we all have the best life! Having a family also forces you to be much more efficient and productive with the time that you <b>are</b> working - and of course, appreciate the precious time you have with each other even more.</p>
					<div align="center">
					<img src="images/family_1.jpg" alt="" width="828" height="397"/> 
					</div>
				</div>
                    <ul class="style1"> 
                        <div id="copyright"> <span>&copy; Darren Cosker. All rights reserved. </span> <span>Design by <a href="http://templated.co" rel="nofollow">TEMPLATED</a>.</span> 
                        </div>                         
                </div>
					
            </div>
